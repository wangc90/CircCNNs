{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aae6c027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f89702f5bf0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import combinations\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "import optuna\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "### import Dataset prepartion and model training classes from BS_LS_scripts folder\n",
    "sys.path.insert(1, '/home/wangc90/circRNA/circRNA_Data/BS_LS_scripts/')\n",
    "from BS_LS_DataSet import BS_LS_DataSet_Prep, BS_LS_upper_lower_concat_rcm\n",
    "\n",
    "random.seed(2023)\n",
    "torch.manual_seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6806dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0475402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1_optuna_10000(nn.Module):\n",
    "    '''\n",
    "        This model take in input sequence 4 X 400 with 1 CNN layer\n",
    "    '''\n",
    "\n",
    "    def __init__(self, trial):\n",
    "\n",
    "        super(Model1_optuna_10000, self).__init__()\n",
    "        ### first CNN layer\n",
    "#         self.out_channel1 = trial.suggest_categorical('out_channel1', [128, 256, 512])\n",
    "        self.out_channel1 = 512\n",
    "#         kernel_size1 = trial.suggest_categorical('kernel_size1', [13, 15, 17, 19, 21])\n",
    "        kernel_size1 = 17\n",
    "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=self.out_channel1, \\\n",
    "                               kernel_size=kernel_size1, stride=1, padding=(kernel_size1 - 1) // 2)\n",
    "        self.conv1_bn = nn.BatchNorm1d(self.out_channel1)\n",
    "#         self.maxpool1 = trial.suggest_categorical('maxpool1', [5, 10, 20])\n",
    "        self.maxpool1 = 5\n",
    "        self.conv1_out_dim = 400 // self.maxpool1\n",
    "\n",
    "#         self.out_channel2 = trial.suggest_categorical('out_channel2', [128, 256, 512])\n",
    "        self.out_channel2 = 512\n",
    "#         kernel_size2 = trial.suggest_categorical('kernel_size2', [13, 15, 17, 19, 21])\n",
    "        kernel_size2 = 21\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.out_channel1, out_channels=self.out_channel2, \\\n",
    "                                   kernel_size=kernel_size2, stride=1, padding=(kernel_size2 - 1) // 2)\n",
    "        self.conv2_bn = nn.BatchNorm1d(self.out_channel2)\n",
    "#         self.maxpool2 = trial.suggest_categorical('maxpool2', [5, 10, 20])\n",
    "        self.maxpool2 = 10\n",
    "        self.conv2_out_dim = 400 // (self.maxpool1 * self.maxpool2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = torch.relu(self.conv1_bn(self.conv1(out)))\n",
    "        out = F.max_pool1d(out, self.maxpool1)\n",
    "        out = torch.relu(self.conv2_bn(self.conv2(out)))\n",
    "        out = F.max_pool1d(out, self.maxpool2)\n",
    "        out = out.view(-1, self.out_channel2 * self.conv2_out_dim)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class ConcatModel1_optuna_10000(nn.Module):\n",
    "    def __init__(self, trial):\n",
    "\n",
    "        super(ConcatModel1_optuna_10000, self).__init__()\n",
    "\n",
    "        ### cnn for the concatenated sequence\n",
    "        self.cnn = Model1_optuna_10000(trial)\n",
    "\n",
    "        # this is for two convlayer\n",
    "        self.out_dim = self.cnn.conv2_out_dim\n",
    "        self.out_channel = self.cnn.out_channel2\n",
    "\n",
    "        self.fc1_input_dim = self.out_channel * self.out_dim\n",
    "\n",
    "#         self.fc1_out = trial.suggest_categorical('concat_fc1_out', [128, 256, 512])\n",
    "        self.fc1_out = 128\n",
    "        self.fc1 = nn.Linear(self.fc1_input_dim, self.fc1_out)\n",
    "\n",
    "        self.fc1_bn = nn.BatchNorm1d(self.fc1_out)\n",
    "\n",
    "#         dropout_rate_fc1 = trial.suggest_categorical(\"concat_dropout_rate_fc1\",  [0, 0.1, 0.2, 0.4])\n",
    "        dropout_rate_fc1 = 0\n",
    "        self.drop_nn1 = nn.Dropout(p=dropout_rate_fc1)\n",
    "\n",
    "        # fc layer2\n",
    "        # use dimension output with nn.CrossEntropyLoss()\n",
    "#         self.fc2_out = trial.suggest_categorical('concat_fc2_out', [4, 8, 16, 32])\n",
    "        self.fc2_out = 4\n",
    "        self.fc2 = nn.Linear(self.fc1_out, self.fc2_out)\n",
    "\n",
    "        self.fc2_bn = nn.BatchNorm1d(self.fc2_out)\n",
    "\n",
    "#         dropout_rate_fc2 = trial.suggest_categorical(\"concat_dropout_rate_fc2\",[0, 0.1, 0.2, 0.4])\n",
    "        dropout_rate_fc2 = 0\n",
    "\n",
    "        self.drop_nn2 = nn.Dropout(p=dropout_rate_fc2)\n",
    "\n",
    "        self.fc3 = nn.Linear(self.fc2_out, 2)\n",
    "\n",
    "    def forward(self, seq_upper_lower_feature):\n",
    "\n",
    "        x = self.cnn(seq_upper_lower_feature)\n",
    "        # feed the concatenated feature to fc1\n",
    "        out = self.fc1(x)\n",
    "        out = self.drop_nn1(torch.relu(self.fc1_bn(out)))\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.drop_nn2(torch.relu(self.fc2_bn(out)))\n",
    "\n",
    "        out = self.fc3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b11325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConcatModel1_optuna_10000_ = ConcatModel1_optuna_10000('trial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe43f11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConcatModel1_optuna_10000(\n",
       "  (cnn): Model1_optuna_10000(\n",
       "    (conv1): Conv1d(4, 512, kernel_size=(17,), stride=(1,), padding=(8,))\n",
       "    (conv1_bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv1d(512, 512, kernel_size=(21,), stride=(1,), padding=(10,))\n",
       "    (conv2_bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=4096, out_features=128, bias=True)\n",
       "  (fc1_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drop_nn1): Dropout(p=0, inplace=False)\n",
       "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
       "  (fc2_bn): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drop_nn2): Dropout(p=0, inplace=False)\n",
       "  (fc3): Linear(in_features=4, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConcatModel1_optuna_10000_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37a96094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retraining(model, dataset, model_folder):\n",
    "    \n",
    "    device = torch.device('cuda') if torch. cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    batch_size = 32\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "#     print(len(train_loader))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model = model('trial').to(device=device)\n",
    "#     print(model)\n",
    "    \n",
    "    \n",
    "    optimizer_name = 'Adam'\n",
    "    lr = 0.00016663145779864379\n",
    "    l2_lambda = 4.828346469350033e-07\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr, weight_decay=l2_lambda)\n",
    "    \n",
    "    epochs = 150 ### reduce the epochs from 150 to 100 to reduce the potential overfitting\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #         print(f\"I'am in the epoch {epoch}\")\n",
    "        model.train()\n",
    "        # record the training loss\n",
    "        running_loss = 0.0\n",
    "\n",
    "        ## deal with different number of features in different dataset with star* notation\n",
    "        for *features, train_labels in train_loader:\n",
    "            ### this line is just for nn.CrossEntropy loss otherwise can be safely removed\n",
    "            train_labels = train_labels.type(torch.LongTensor)\n",
    "\n",
    "            train_labels = train_labels.to(device)\n",
    "            features = [i.to(device) for i in features]\n",
    "\n",
    "            # forward pass\n",
    "            train_preds = model(*features)\n",
    "            loss = criterion(train_preds, train_labels)\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()  # empty the gradient from last round\n",
    "\n",
    "            # calculate the gradient\n",
    "            loss.backward()\n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        #         print(f\"I'am finished the epoch {epoch} training\")\n",
    "        ## start model validation\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # first evaluate the training acc ## don't need to evaluate the training acc\n",
    "            correct, total = 0.0, 0.0\n",
    "            for *features, train_labels in train_loader:\n",
    "                ### this type conversion is just used for nn.CrossEntropy loss\n",
    "                ### otherwise can be safely removed\n",
    "                train_labels = train_labels.to(device)\n",
    "                features = [i.to(device) for i in features]\n",
    "            \n",
    "                # get the predition with the model parameters updated after each epoch\n",
    "                preds = model(*features)\n",
    "                # prediction for the nn.CrossEntropy loss\n",
    "                _, preds_labels = torch.max(preds, 1)\n",
    "                correct += (preds_labels == train_labels).sum().item()\n",
    "                total += train_labels.shape[0]\n",
    "            \n",
    "            train_acc = round(correct / total, 4)\n",
    "            \n",
    "        print(f\"I'am finished the epoch {epoch} evaluation on the training set\")\n",
    "            \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f'epoch {epoch + 1}, training loss {running_loss}, train accuracy {train_acc}')\n",
    "    \n",
    "    # save the model at the end of 150 epochs\n",
    "    model_path = f\"{model_folder}/retrained_model_{epoch}.pt\"\n",
    "    \n",
    "    torch.save(model, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f1b430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "132b3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model1_10000_retraining():\n",
    "    ### where to save the 3-fold CV validation acc\n",
    "\n",
    "    ### where to save the retrained model\n",
    "    model_folder = '/home/wangc90/circRNA/circRNA_Data/model_outputs/Base_model1_retraining/Base_model1_retraining_10000'\n",
    "\n",
    "    ## These need to be changed for redhawks\n",
    "    BS_LS_coordinates_path = '/home/wangc90/circRNA/circRNA_Data/BS_LS_data/updated_data/BS_LS_coordinates_final.csv'\n",
    "    hg19_seq_dict_json_path = '/home/wangc90/circRNA/circRNA_Data/hg19_seq/hg19_seq_dict.json'\n",
    "    flanking_dict_folder = '/home/wangc90/circRNA/circRNA_Data/BS_LS_data/flanking_dicts/'\n",
    "    bs_ls_dataset = BS_LS_DataSet_Prep(BS_LS_coordinates_path=BS_LS_coordinates_path,\n",
    "                                       hg19_seq_dict_json_path=hg19_seq_dict_json_path,\n",
    "                                       flanking_dict_folder=flanking_dict_folder,\n",
    "                                       flanking_junction_bps=100,\n",
    "                                       flanking_intron_bps=100,\n",
    "                                       training_size=10000)\n",
    "\n",
    "    ### generate the junction and flanking intron dict\n",
    "    bs_ls_dataset.get_junction_flanking_intron_seq()\n",
    "\n",
    "    ### use the 10000 for training RCM and junction seq and use 1000 for combine them\n",
    "    train_key1, _, test_keys = bs_ls_dataset.get_train_test_keys()\n",
    "\n",
    "\n",
    "    rcm_scores_folder = '/home/wangc90/Desktop/project_data/flanking_dicts/rcm_scores/'\n",
    "\n",
    "    ## try without rcm features\n",
    "    train_torch_upper_lower_features, train_torch_labels = bs_ls_dataset.seq_to_tensor(data_keys=train_key1,\n",
    "                                                                                       rcm_folder=rcm_scores_folder,\n",
    "                                                                                       is_rcm=False,\n",
    "                                                                                       is_upper_lower_concat=True)\n",
    "\n",
    "    BS_LS_dataset = BS_LS_upper_lower_concat_rcm(include_rcm=False,\n",
    "                                          seq_upper_lower_feature=train_torch_upper_lower_features,\n",
    "                                          flanking_rcm=None,\n",
    "                                          upper_rcm=None,\n",
    "                                          lower_rcm=None,\n",
    "                                          label=train_torch_labels)\n",
    "    print(len(BS_LS_dataset))\n",
    "\n",
    "\n",
    "    retraining(model=ConcatModel1_optuna_10000, dataset=BS_LS_dataset,model_folder=model_folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eab9310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model1_10000_retraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c3376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1873e8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40d2ebc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr5|138837130|138837392|- has N in the extracted junctions, belongs to BS\n",
      "There are 0 overlapped flanking sequence from BS and LS  \n",
      "There are 7 repeated BS sequences\n",
      "There are 2 repeated LS sequences\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2216"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## These need to be changed for redhawks\n",
    "BS_LS_coordinates_path = '/home/wangc90/circRNA/circRNA_Data/BS_LS_data/updated_data/BS_LS_coordinates_final.csv'\n",
    "hg19_seq_dict_json_path = '/home/wangc90/circRNA/circRNA_Data/hg19_seq/hg19_seq_dict.json'\n",
    "flanking_dict_folder = '/home/wangc90/circRNA/circRNA_Data/BS_LS_data/flanking_dicts/'\n",
    "bs_ls_dataset = BS_LS_DataSet_Prep(BS_LS_coordinates_path=BS_LS_coordinates_path,\n",
    "                                   hg19_seq_dict_json_path=hg19_seq_dict_json_path,\n",
    "                                   flanking_dict_folder=flanking_dict_folder,\n",
    "                                   flanking_junction_bps=100,\n",
    "                                   flanking_intron_bps=100,\n",
    "                                   training_size=10000)\n",
    "\n",
    "### generate the junction and flanking intron dict\n",
    "bs_ls_dataset.get_junction_flanking_intron_seq()\n",
    "\n",
    "### use the 10000 for training RCM and junction seq and use 1000 for combine them\n",
    "train_key1, _, test_keys = bs_ls_dataset.get_train_test_keys()\n",
    "\n",
    "\n",
    "rcm_scores_folder = '/home/wangc90/Desktop/project_data/flanking_dicts/rcm_scores/'\n",
    "\n",
    "## try without rcm features\n",
    "test_torch_upper_lower_features, test_torch_labels = bs_ls_dataset.seq_to_tensor(data_keys=test_keys,\n",
    "                                                                                   rcm_folder=rcm_scores_folder,\n",
    "                                                                                   is_rcm=False,\n",
    "                                                                                   is_upper_lower_concat=True)\n",
    "\n",
    "BS_LS_dataset = BS_LS_upper_lower_concat_rcm(include_rcm=False,\n",
    "                                      seq_upper_lower_feature=test_torch_upper_lower_features,\n",
    "                                      flanking_rcm=None,\n",
    "                                      upper_rcm=None,\n",
    "                                      lower_rcm=None,\n",
    "                                      label=test_torch_labels)\n",
    "\n",
    "len(BS_LS_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a54e1375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_eva(model_path, test_data_set):\n",
    "    \n",
    "    '''\n",
    "    ### load the saved best model and do the evaluation on the independent test dataset\n",
    "\n",
    "    '''\n",
    "    saved_model = torch.load(model_path).to('cuda')\n",
    "\n",
    "    saved_model.eval()\n",
    "\n",
    "    # test_data_set = torch.utils.data.Subset(BS_LS_test_dataset)\n",
    "\n",
    "    data_loader = DataLoader(test_data_set, batch_size=100)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        all_test_labels = []\n",
    "        all_preds_prob = []\n",
    "\n",
    "        correct, total = 0.0, 0.0\n",
    "        \n",
    "        for *test_features, test_labels in data_loader:\n",
    "            #### change it to cuda:1 when evaluation for rcm models\n",
    "            test_labels = test_labels.type(torch.LongTensor).to('cuda')\n",
    "            test_features = [i.to('cuda') for i in test_features]\n",
    "\n",
    "            preds = saved_model(*test_features)\n",
    "            ## get the predited probability\n",
    "            preds_prob = F.softmax(preds, dim=1)[:, 1]\n",
    "\n",
    "            _, preds_labels = torch.max(preds, 1)\n",
    "            correct += (preds_labels == test_labels).sum().item()\n",
    "            total += test_labels.shape[0]\n",
    "\n",
    "            all_test_labels.extend(test_labels.cpu().numpy().tolist())\n",
    "            all_preds_prob.extend(preds_prob.cpu().numpy().tolist())\n",
    "\n",
    "        val_acc = round(correct / total, 4)\n",
    "\n",
    "        print(val_acc)\n",
    "        \n",
    "    return torch.from_numpy(np.array(all_test_labels)), torch.from_numpy(np.array(all_preds_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f19fe643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8673\n"
     ]
    }
   ],
   "source": [
    "model1_test_labels, model1_preds_prob = Model_eva(model_path='/home/wangc90/circRNA/circRNA_Data/model_outputs/Base_model1_retraining/Base_model1_retraining_10000/retrained_model_149.pt',\\\n",
    "                                                  test_data_set=BS_LS_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a369123c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
