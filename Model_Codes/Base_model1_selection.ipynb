{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08e14640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import combinations\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "import optuna\n",
    "from torchmetrics.classification import F1Score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36318be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BS_LS_DataSet_Prep():\n",
    "    def __init__(self, BS_LS_coordinates_path, hg19_seq_dict_json_path,\n",
    "                 flanking_dict_folder,\n",
    "                 flanking_junction_bps=None,\n",
    "                 flanking_intron_bps=None,\n",
    "                 training_size=None):\n",
    "\n",
    "        self.BS_LS_coordinates_path = BS_LS_coordinates_path\n",
    "        self.flanking_junction_bps = flanking_junction_bps\n",
    "        self.flanking_intron_bps = flanking_intron_bps\n",
    "        self.flanking_dict_folder = flanking_dict_folder\n",
    "        ### bring in the hg19 genomic sequences\n",
    "        with open(hg19_seq_dict_json_path) as f:\n",
    "            self.hg19_seq_dict = json.load(f)\n",
    "        self.junction_seq_json_name = f'BS_LS_junction_seq_{self.flanking_junction_bps}_bps'\n",
    "        self.flanking_seq_json_name = f'BS_LS_intronic_flanking_seq_{self.flanking_intron_bps}_bps'\n",
    "        self.training_size = training_size\n",
    "\n",
    "    def reverse_complement(self, input_seq):\n",
    "        '''\n",
    "        This function take a sequence and returns the reverse complementary sequence\n",
    "        '''\n",
    "        complement_dict = {'A': 'T', 'G': 'C', 'T': 'A', 'C': 'G', 'N': 'N'}\n",
    "        out_seq = ''.join([complement_dict[base] for base in list(input_seq)[::-1]])\n",
    "        return out_seq\n",
    "\n",
    "    def get_junction_flanking_intron_seq(self):\n",
    "        '''\n",
    "        :param BS_LS_coordinates_path:\n",
    "        :param flanking_bps:\n",
    "        :return: two dictionaries\n",
    "        1) junction_seq dictionary that stores the upper_intron, upper_exon, lower_exon, and lower_intron\n",
    "        sequences with 50bps (default) each for a given LS or BS site with the key:\n",
    "        chr|up_end|start|end|down_start|strand\n",
    "\n",
    "        2) intronic_flanking_seq dictionary stores the flanking intronic sequence for the exon pairs with the key:\n",
    "        chr|start|end|strand\n",
    "        '''\n",
    "\n",
    "        junction_seq = {}\n",
    "        intronic_flanking_seq = {}\n",
    "\n",
    "        ## retain the BS exons that have no valid boundaries\n",
    "        BS_LS_coordinates_df = pd.read_csv(self.BS_LS_coordinates_path, sep='\\t')\n",
    "        for _, row in BS_LS_coordinates_df.iterrows():\n",
    "            chrom, strand, start, end, label = row['chr'], row['strand'], int(row['start']), int(row['end']),\\\n",
    "                                                                   row['Splicing_type']\n",
    "            # this key is unique for each instance in the BS_LS dataframe\n",
    "            key = '|'.join([chrom, str(start), str(end), strand])\n",
    "\n",
    "            # get the corresponding chromosome DNA sequence\n",
    "            dna_seq = self.hg19_seq_dict[chrom]\n",
    "            # extract the spliced genomic sequence assuming the positive strand\n",
    "            spliced_seq_P = dna_seq[start: end].upper()\n",
    "            # extract the upper_intron junction seq assuming the positive strand\n",
    "\n",
    "            ### use 200 introns here by addition of 100bps, remove 100 to get the original data\n",
    "            # upper_intron_P = dna_seq[start - self.flanking_bps-100: start].upper()\n",
    "            upper_intron_P = dna_seq[start - self.flanking_junction_bps: start].upper()\n",
    "\n",
    "            # extract the lower_intron junction seq assuming the positive strand\n",
    "            # lower_intron_P = dna_seq[end: end + self.flanking_bps+100].upper()\n",
    "            lower_intron_P = dna_seq[end: end + self.flanking_junction_bps].upper()\n",
    "\n",
    "            # skip the rows that have 'Ns' in the upper_intron or lower_intron\n",
    "            if 'N' in upper_intron_P or 'N' in lower_intron_P:\n",
    "                print(f'{key} has N in the extracted junctions, belongs to {label}')\n",
    "                continue\n",
    "\n",
    "            # get the upper flanking sequence for the positive strand: instead of\n",
    "            # using the up_end position use the start - 1000 as the starting point\n",
    "\n",
    "            U_flanking_seq_P = dna_seq[start-self.flanking_intron_bps:start].upper()\n",
    "\n",
    "            # get the lower flanking sequence using down_start + 1000 as the ending point\n",
    "            L_flanking_seq_P = dna_seq[end:end+self.flanking_intron_bps].upper()\n",
    "\n",
    "            # consider if the strand is -\n",
    "            if strand == '-':\n",
    "                ### get the junction sequence\n",
    "                spliced_seq_N = self.reverse_complement(spliced_seq_P)\n",
    "                upper_exon_N = spliced_seq_N[:self.flanking_junction_bps]\n",
    "                lower_exon_N = spliced_seq_N[-self.flanking_junction_bps:]\n",
    "\n",
    "                upper_intron_N = self.reverse_complement(lower_intron_P)\n",
    "                lower_intron_N = self.reverse_complement(upper_intron_P)\n",
    "\n",
    "                ### get the flanking intronic sequence\n",
    "                U_flanking_seq = self.reverse_complement(L_flanking_seq_P)\n",
    "                L_flanking_seq = self.reverse_complement(U_flanking_seq_P)\n",
    "\n",
    "                intronic_flanking_seq[key] = {'U_flanking_seq': U_flanking_seq,\n",
    "                                              'L_flanking_seq': L_flanking_seq,\n",
    "                                              'label': label}\n",
    "\n",
    "                junction_seq[key] = {'spliced_seq': spliced_seq_N,\n",
    "                                     'upper_intron_' + str(self.flanking_junction_bps): upper_intron_N,\n",
    "                                     'upper_exon_' + str(self.flanking_junction_bps): upper_exon_N,\n",
    "                                     'lower_exon_' + str(self.flanking_junction_bps): lower_exon_N,\n",
    "                                     'lower_intron_' + str(self.flanking_junction_bps): lower_intron_N,\n",
    "                                     'label': label}\n",
    "\n",
    "            else:\n",
    "                upper_exon_P = spliced_seq_P[:self.flanking_junction_bps]\n",
    "\n",
    "                lower_exon_P = spliced_seq_P[-self.flanking_junction_bps:]\n",
    "\n",
    "                junction_seq[key] = {'spliced_seq': spliced_seq_P,\n",
    "                                     'upper_intron_' + str(self.flanking_junction_bps): upper_intron_P,\n",
    "                                     'upper_exon_' + str(self.flanking_junction_bps): upper_exon_P,\n",
    "                                     'lower_exon_' + str(self.flanking_junction_bps): lower_exon_P,\n",
    "                                     'lower_intron_' + str(self.flanking_junction_bps): lower_intron_P,\n",
    "                                     'label': label}\n",
    "\n",
    "                intronic_flanking_seq[key] = {'U_flanking_seq': U_flanking_seq_P,\n",
    "                                              'L_flanking_seq': L_flanking_seq_P,\n",
    "                                              'label': label}\n",
    "\n",
    "        ### remove the repeative sequence from the junction_seq dictionary and then\n",
    "        ### remove the overlapping sequence between BS and LS\n",
    "        BS_junction_seq_dict = {}\n",
    "        LS_junction_seq_dict = {}\n",
    "\n",
    "        for i, j in junction_seq.items():\n",
    "            if j['label'] == 'BS':\n",
    "                BS_junction_seq_dict[i] = j['upper_intron_' + str(self.flanking_junction_bps)] + \\\n",
    "                                          j['upper_exon_' + str(self.flanking_junction_bps)] + \\\n",
    "                                          j['lower_exon_' + str(self.flanking_junction_bps)] + \\\n",
    "                                          j['lower_intron_' + str(self.flanking_junction_bps)]\n",
    "            else:\n",
    "                LS_junction_seq_dict[i] = j['upper_intron_' + str(self.flanking_junction_bps)] + \\\n",
    "                                          j['upper_exon_' + str(self.flanking_junction_bps)] + \\\n",
    "                                          j['lower_exon_' + str(self.flanking_junction_bps)] + \\\n",
    "                                          j['lower_intron_' + str(self.flanking_junction_bps)]\n",
    "\n",
    "        # first get the 43 overlapping junction sequences and then filter the two dict from these sequences\n",
    "        overlap_junction_seqs = set(BS_junction_seq_dict.values()).intersection(LS_junction_seq_dict.values())\n",
    "\n",
    "        # remove these 43 overlapping junction sequences from BS_junction_seq_dict\n",
    "        BS_junction_seq_dict_wo_overlap = {key: value for key, value in BS_junction_seq_dict.items() if\n",
    "                                           value not in overlap_junction_seqs}\n",
    "\n",
    "        # remove the duplicated junction sequences from BS by using the value as the key and then reverse it\n",
    "        BS_tem_dict = {value: key for key, value in BS_junction_seq_dict_wo_overlap.items()}\n",
    "        BS_res_dict = {value: key for key, value in BS_tem_dict.items()}\n",
    "\n",
    "        # remove these 43 overlapping junction sequences from LS_junction_seq_dict\n",
    "        LS_junction_seq_dict_wo_overlap = {key: value for key, value in LS_junction_seq_dict.items() if\n",
    "                                           value not in overlap_junction_seqs}\n",
    "\n",
    "        # remove the duplicated junction sequences from LS by using the value as the key and then reverse it\n",
    "        LS_tem_dict = {value: key for key, value in LS_junction_seq_dict_wo_overlap.items()}\n",
    "        LS_res_dict = {value: key for key, value in LS_tem_dict.items()}\n",
    "\n",
    "        # merge two dicts\n",
    "        BS_LS_res_dict = {**BS_res_dict, **LS_res_dict}\n",
    "\n",
    "        # filter the junction_seq with the new keys from BS_LS_res_dict\n",
    "        junction_seq_final = {key: junction_seq[key] for key in BS_LS_res_dict.keys()}\n",
    "\n",
    "        # filter the intronic_flanking_seq with the new keys from BS_LS_res_dict\n",
    "        intronic_flanking_seq_final = {key: intronic_flanking_seq[key] for key in BS_LS_res_dict.keys()}\n",
    "\n",
    "        ## save the junction_seq and intronic_flanking_seq to json on the harddrive\n",
    "\n",
    "        with open(f'{self.flanking_dict_folder}{self.junction_seq_json_name}.json', 'w') as f:\n",
    "            json.dump(junction_seq_final, f)\n",
    "\n",
    "        with open(f'{self.flanking_dict_folder}{self.flanking_seq_json_name}.json', 'w') as f:\n",
    "            json.dump(intronic_flanking_seq_final, f)\n",
    "\n",
    "    #         return junction_seq, intronic_flanking_seq\n",
    "\n",
    "    def get_train_test_keys(self):\n",
    "\n",
    "        if not os.path.exists(f'{self.flanking_dict_folder}{self.junction_seq_json_name}.json'):\n",
    "            # invoke the function to write the BS_LS_junction_seq and BS_LS_intronic_flanking_seq to the drive\n",
    "            self.get_junction_flanking_intron_seq()\n",
    "            with open(f'{self.flanking_dict_folder}{self.junction_seq_json_name}.json') as f:\n",
    "                BS_LS_junction_seq_dict = json.load(f)\n",
    "        else:\n",
    "            # read the BS_LS_junction_seq from harddrive instead of calling the function\n",
    "            with open(f'{self.flanking_dict_folder}{self.junction_seq_json_name}.json') as f:\n",
    "                BS_LS_junction_seq_dict = json.load(f)\n",
    "\n",
    "        BS_exon_key_list = [key for key, value in BS_LS_junction_seq_dict.items() if value['label'] == 'BS']\n",
    "        LS_exon_key_list = [key for key, value in BS_LS_junction_seq_dict.items() if value['label'] == 'LS']\n",
    "\n",
    "        np.random.seed(42)\n",
    "        # randomly select 12000 from both BS and LS key list and combine them\n",
    "        BS_exon_train_keys = np.random.choice(BS_exon_key_list, size=self.training_size, replace=False)\n",
    "        LS_exon_train_keys = np.random.choice(LS_exon_key_list, size=self.training_size, replace=False)\n",
    "        BS_LS_exon_train_keys = np.concatenate([BS_exon_train_keys, LS_exon_train_keys])\n",
    "\n",
    "        # select the remaining keys from BS and LS key list as the test keys\n",
    "        BS_exon_test_keys = set(BS_exon_key_list).difference(BS_exon_train_keys)\n",
    "        LS_exon_test_keys = set(LS_exon_key_list).difference(LS_exon_train_keys)\n",
    "        BS_LS_exon_test_keys = np.array(list(BS_exon_test_keys.union(LS_exon_test_keys)))\n",
    "\n",
    "        return BS_LS_exon_train_keys, BS_LS_exon_test_keys\n",
    "\n",
    "    def seq_to_matrix(self, input_seq):\n",
    "        '''\n",
    "            This function takes a DNA sequence and return a one-hot encoded matrix of 4 X N (length of input_seq)\n",
    "        '''\n",
    "        row_index = {'A': 0, 'G': 1, 'C': 2, 'T': 3}  # should exclude the 'Ns' in the input sequence\n",
    "\n",
    "        # initialize the 4 X N 0 matrix:\n",
    "        input_mat = np.zeros((4, len(input_seq)))\n",
    "\n",
    "        for col_index, base in enumerate(input_seq):\n",
    "            input_mat[row_index[base]][col_index] = 1\n",
    "        return input_mat\n",
    "    \n",
    "    #### create all sequence features, rcm_features and a2i features silmaltineous here\n",
    "    def seq_to_tensor(self, data_keys, rcm_folder, is_rcm=None, is_upper_lower_concat=None):\n",
    "        '''\n",
    "        :param data_keys:\n",
    "        :param rcm_folder\n",
    "        :param is_rcm: boolean value indicate if rcm features is genearated or not\n",
    "        :param is_upper_lower_concat: boolean value indicates if upper and lower junction is concatenate or not\n",
    "        :return: concatenated 2-d data for upper and lower seq for all the keys in the data_keys\n",
    "        '''\n",
    "        ### first get the BS_LS_junction_seq_dict;\n",
    "        if not os.path.exists(f'{self.flanking_dict_folder}{self.junction_seq_json_name}.json'):\n",
    "            self.get_junction_flanking_intron_seq()\n",
    "            with open(f'{self.flanking_dict_folder}{self.junction_seq_json_name}.json') as f:\n",
    "                BS_LS_junction_seq_dict = json.load(f)\n",
    "        else:\n",
    "            with open(f'{self.flanking_dict_folder}{self.junction_seq_json_name}.json') as f:\n",
    "                BS_LS_junction_seq_dict = json.load(f)\n",
    "\n",
    "        ### then get the BS_LS_flanking_seq_dict;\n",
    "        if not os.path.exists(f'{self.flanking_dict_folder}{self.flanking_seq_json_name}.json'):\n",
    "            self.get_junction_flanking_intron_seq()\n",
    "            with open(f'{self.flanking_dict_folder}{self.flanking_seq_json_name}.json') as f:\n",
    "                BS_LS_flanking_seq_dict = json.load(f)\n",
    "        else:\n",
    "            with open(f'{self.flanking_dict_folder}{self.flanking_seq_json_name}.json') as f:\n",
    "                BS_LS_flanking_seq_dict = json.load(f)\n",
    "\n",
    "        ### these two dictionary use the same keys\n",
    "        ### list to store the concatenated upper and lower sequence one-hot encoding\n",
    "        if is_upper_lower_concat:\n",
    "            all_torch_feature_list = []\n",
    "            \n",
    "        ### list to store the upper and lower sequence one-hot encoding separately \n",
    "        else:\n",
    "            all_torch_upper_feature_list = []\n",
    "            all_torch_lower_feature_list = []\n",
    "            \n",
    "        ### list to store rcm features if is_rcm true ####using for loops!!!!!!\n",
    "        if is_rcm:\n",
    "            ### tri-cnn for flanking, upper and lower separately\n",
    "            flanking_rcm_scores = []\n",
    "            upper_rcm_scores = []\n",
    "            lower_rcm_scores = []\n",
    "            \n",
    "            \n",
    "            flanking_rcm_dict_list = []\n",
    "            upper_rcm_dict_list = []\n",
    "            lower_rcm_dict_list = []\n",
    "            \n",
    "            seed_len_list = [5, 7, 9, 11, 13]\n",
    "            \n",
    "#             for flanking_intron_len in [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000,\n",
    "#                                         1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800,\n",
    "#                                         1900, 2000]:\n",
    "            for flanking_intron_len in [100, 500, 1000,1500, 2000]:\n",
    "                \n",
    "                for seed_len in seed_len_list:\n",
    "            \n",
    "                    with open(os.path.join(rcm_scores_folder, f'to_519_rcm_flanking_{flanking_intron_len}_bps_introns_{seed_len}mer.json')) as f:\n",
    "                        flanking_rcm_dict = json.load(f)\n",
    "                        flanking_rcm_dict_list.append(flanking_rcm_dict)\n",
    "                    with open(os.path.join(rcm_scores_folder, f'to_519_rcm_upper_{flanking_intron_len}_bps_introns_{seed_len}mer.json')) as f:\n",
    "                        upper_rcm_dict = json.load(f)\n",
    "                        upper_rcm_dict_list.append(upper_rcm_dict)\n",
    "                    with open(os.path.join(rcm_scores_folder, f'to_519_rcm_lower_{flanking_intron_len}_bps_introns_{seed_len}mer.json')) as f:\n",
    "                        lower_rcm_dict = json.load(f)\n",
    "                        lower_rcm_dict_list.append(lower_rcm_dict)\n",
    "  \n",
    "            \n",
    "        ### list to store the corresponding labels: LS or BS\n",
    "        all_label_list = []\n",
    "\n",
    "\n",
    "        for key in data_keys:\n",
    "            ### get the rcm features from the harddrive\n",
    "            ### should produce the RCM features simultaneously with the seq feture for the same data point\n",
    "\n",
    "            flanking_seqs = BS_LS_flanking_seq_dict[key]\n",
    "\n",
    "            ## construct the rcm feature if is_rcm is True\n",
    "            if is_rcm:\n",
    "                \n",
    "                flanking_rcm_kmer_list = [np.log(np.array(flanking_rcm[key][0]).reshape(5,5)+1) for flanking_rcm in flanking_rcm_dict_list]\n",
    "                flanking_rcm_kmers = torch.from_numpy(np.concatenate(flanking_rcm_kmer_list, axis=1)).to(torch.float32)\n",
    "                          \n",
    "                upper_rcm_kmer_list = [np.log(np.array(upper_rcm[key][0]).reshape(5,5)+1) for upper_rcm in upper_rcm_dict_list]\n",
    "                upper_rcm_kmers = torch.from_numpy(np.concatenate(upper_rcm_kmer_list, axis=1)).to(torch.float32)\n",
    "                          \n",
    "                lower_rcm_kmer_list = [np.log(np.array(lower_rcm[key][0]).reshape(5,5)+1) for lower_rcm in lower_rcm_dict_list]\n",
    "                lower_rcm_kmers = torch.from_numpy(np.concatenate(lower_rcm_kmer_list, axis=1)).to(torch.float32)\n",
    "                          \n",
    "                \n",
    "#                 rcm_values_concate = torch.from_numpy(np.stack([flanking_rcm_kmers,\n",
    "#                                                             upper_rcm_kmers,\n",
    "#                                                             lower_rcm_kmers], axis=0)).to(torch.float32)\n",
    "                \n",
    "                flanking_rcm_scores.append(flanking_rcm_kmers)\n",
    "                upper_rcm_scores.append(upper_rcm_kmers)\n",
    "                lower_rcm_scores.append(lower_rcm_kmers)\n",
    "                \n",
    "                \n",
    "            ### working with the junction_seq starting here\n",
    "            value = BS_LS_junction_seq_dict[key]\n",
    "            # extract the poisitve (BS: as 1) and negative label (LS: as 0)\n",
    "\n",
    "            ## make sure the label are the same for the same key and append the labe to the list\n",
    "            label = value['label']\n",
    "            assert label == flanking_seqs['label'], f\"Same sequence key {key} with different labels\"\n",
    "\n",
    "            if label == 'BS':\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "                \n",
    "            all_label_list.append(label)\n",
    "\n",
    "            # concatenate the upper seq together and lower seq together for two separate CNN to process\n",
    "            concatenated_upper_seq = value['upper_intron_{}'.format(str(self.flanking_junction_bps))] + \\\n",
    "                               value['upper_exon_{}'.format(str(self.flanking_junction_bps))]\n",
    "            concatenated_lower_seq = value['lower_exon_{}'.format(str(self.flanking_junction_bps))] + \\\n",
    "                               value['lower_intron_{}'.format(str(self.flanking_junction_bps))]\n",
    "            \n",
    "            ### test whether want to concatenate the upper seq and lower seq together\n",
    "            if is_upper_lower_concat:\n",
    "                \n",
    "                upper_lower_concat_seq = concatenated_upper_seq + concatenated_lower_seq\n",
    "                upper_lower_concat_mat = self.seq_to_matrix(upper_lower_concat_seq)\n",
    "                individual_upper_lower_torch = torch.from_numpy(upper_lower_concat_mat).to(torch.float32)\n",
    "                all_torch_feature_list.append(individual_upper_lower_torch)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                individual_upper_mat = self.seq_to_matrix(concatenated_upper_seq)\n",
    "                individual_lower_mat = self.seq_to_matrix(concatenated_lower_seq)\n",
    "\n",
    "                # convert individual instance to torch\n",
    "                individual_upper_torch = torch.from_numpy(individual_upper_mat).to(torch.float32)\n",
    "                individual_lower_torch = torch.from_numpy(individual_lower_mat).to(torch.float32)\n",
    "\n",
    "                all_torch_upper_feature_list.append(individual_upper_torch)\n",
    "                all_torch_lower_feature_list.append(individual_lower_torch)\n",
    "                \n",
    "                \n",
    "        all_torch_label = torch.tensor(all_label_list, dtype=torch.float32) #.view(-1, 1)\n",
    "        \n",
    "        if is_rcm:\n",
    "            all_torch_flanking_rcm_features = torch.stack(flanking_rcm_scores, dim=0)\n",
    "            all_torch_upper_rcm_features = torch.stack(upper_rcm_scores, dim=0)\n",
    "            all_torch_lower_rcm_features = torch.stack(lower_rcm_scores, dim=0)\n",
    "            \n",
    "            \n",
    "#             all_torch_rcm_feature = torch.stack(rcm_scores, dim=0)\n",
    "            \n",
    "        if is_upper_lower_concat:\n",
    "            \n",
    "            all_torch_feature = torch.stack(all_torch_feature_list, dim=0)\n",
    "            \n",
    "        else:\n",
    "            all_torch_upper_feature = torch.stack(all_torch_upper_feature_list, dim=0)\n",
    "            all_torch_lower_feature = torch.stack(all_torch_lower_feature_list, dim=0)\n",
    "            \n",
    "        ### return the tensor based on several requirement\n",
    "        ## return only the upper, lower and rcm / upper lower, a2i / or both/ or just upper lower /or just concate\n",
    "        \n",
    "        if is_upper_lower_concat and not is_rcm:\n",
    "            \n",
    "            return all_torch_feature, all_torch_label\n",
    "        \n",
    "        if is_upper_lower_concat and is_rcm:\n",
    "            return all_torch_feature, all_torch_flanking_rcm_features, all_torch_upper_rcm_features, all_torch_lower_rcm_features, all_torch_label\n",
    "        \n",
    "        if not is_upper_lower_concat and not is_rcm:\n",
    "            return all_torch_upper_feature, all_torch_lower_feature, all_torch_label\n",
    "        \n",
    "        if not is_upper_lower_concat and is_rcm:\n",
    "            return all_torch_upper_feature, all_torch_lower_feature, all_torch_flanking_rcm_features, all_torch_upper_rcm_features, all_torch_lower_rcm_features, all_torch_label\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf99d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ad3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BS_LS_upper_lower_concat_rcm(Dataset):\n",
    "    def __init__(self, include_rcm, seq_upper_lower_feature, flanking_rcm, upper_rcm, lower_rcm, label):\n",
    "        # construction of the map-style datasets\n",
    "        # data loading\n",
    "        self.include_rcm = include_rcm\n",
    "        \n",
    "        if self.include_rcm:\n",
    "            self.x1 = seq_upper_lower_feature\n",
    "\n",
    "            self.x2 = flanking_rcm\n",
    "            self.x3 = upper_rcm\n",
    "            self.x4 = lower_rcm\n",
    "            \n",
    "        else:\n",
    "            self.x1 = seq_upper_lower_feature\n",
    "            \n",
    "        self.y = label\n",
    "\n",
    "        self.n_samples = seq_upper_lower_feature.size()[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # dataset[0]\n",
    "        if self.include_rcm:\n",
    "            return self.x1[index], self.x2[index], self.x3[index], self.x4[index], self.y[index]\n",
    "        else:\n",
    "            return self.x1[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        # len(dataset)\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16bc7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model 1 input sequence 4 X 400 with 1 or 2CNN layer\n",
    "### remove the dropout for CNN layers\n",
    "\n",
    "class Model1_optuna(nn.Module):\n",
    "    '''\n",
    "        This model take in input sequence 4 X 400 with 1 CNN layer\n",
    "    '''\n",
    "    def __init__(self, trial, cnn_num):\n",
    "        \n",
    "        self.cnn_num = cnn_num\n",
    "        \n",
    "        super(Model1_optuna, self).__init__()\n",
    "        \n",
    "        ### first CNN layer\n",
    "#         self.out_channel1 = trial.suggest_categorical('out_channel1', [32, 64, 128, 256, 512])\n",
    "        self.out_channel1 = 512\n",
    "#         kernel_size1 = trial.suggest_categorical('kernel_size1', [13, 15, 17, 19, 21])\n",
    "        kernel_size1 = 13\n",
    "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=self.out_channel1,\\\n",
    "                               kernel_size=kernel_size1, stride=1, padding=(kernel_size1 - 1) // 2)\n",
    "\n",
    "        self.conv1_bn = nn.BatchNorm1d(self.out_channel1)\n",
    "\n",
    "#         dropout_rate_conv1 = trial.suggest_categorical(\"dropout_rate_conv1\", [0, 0.1, 0.2, 0.4, 0.8])\n",
    "#         dropout_rate_conv1 = 0\n",
    "#         self.drop_conv1 = nn.Dropout(p=dropout_rate_conv1)\n",
    "\n",
    "#         self.maxpool1 = trial.suggest_categorical('maxpool1', [5, 10, 20])\n",
    "        self.maxpool1 = 5\n",
    "        self.conv1_out_dim = 400 // self.maxpool1\n",
    "        \n",
    "        if self.cnn_num == 2:\n",
    "        \n",
    "            ### second CNN layer\n",
    "\n",
    "#             self.out_channel2 = trial.suggest_categorical('out_channel2', [32, 64, 128, 256, 512])\n",
    "            self.out_channel2 = 512\n",
    "#             kernel_size2 = trial.suggest_categorical('kernel_size2', [13, 15, 17, 19, 21])\n",
    "            kernel_size2 = 21\n",
    "\n",
    "            self.conv2 = nn.Conv1d(in_channels= self.out_channel1, out_channels=self.out_channel2,\\\n",
    "                                   kernel_size=kernel_size2, stride=1, padding=(kernel_size2 - 1) // 2)\n",
    "\n",
    "            self.conv2_bn = nn.BatchNorm1d(self.out_channel2)\n",
    "\n",
    "#             dropout_rate_conv2 = trial.suggest_categorical(\"dropout_rate_conv2\", [0, 0.1, 0.2, 0.4, 0.8])\n",
    "#             dropout_rate_conv2 = 0\n",
    "#             self.drop_conv2 = nn.Dropout(p=dropout_rate_conv2)\n",
    "\n",
    "#             self.maxpool2 = trial.suggest_categorical('maxpool2', [5, 10, 20])\n",
    "            self.maxpool2 = 5\n",
    "            self.conv2_out_dim = 400 // (self.maxpool1 * self.maxpool2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = torch.relu(self.conv1_bn(self.conv1(out)))\n",
    "        out = F.max_pool1d(out, self.maxpool1)\n",
    "        if self.cnn_num == 2:\n",
    "            out = torch.relu(self.conv2_bn(self.conv2(out)))\n",
    "            out = F.max_pool1d(out, self.maxpool2)\n",
    "            out = out.view(-1, self.out_channel2 * self.conv2_out_dim)\n",
    "        else:\n",
    "            out = out.view(-1, self.out_channel1 * self.conv1_out_dim)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RCM_optuna_flanking(nn.Module):\n",
    "    '''\n",
    "        This is for 2-d model to process the RCM score distribution of the flanking introns\n",
    "    '''\n",
    "    def __init__(self, trial):\n",
    "        \n",
    "        super(RCM_optuna_flanking, self).__init__()\n",
    "        \n",
    "        # convlayer 1\n",
    "#         self.out_channel1 = trial.suggest_categorical('RCM_flanking_out_channel1', [32, 64, 128, 256, 512])\n",
    "        self.out_channel1 = 128\n",
    "#         kernel_size1 = 5\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=5, out_channels=self.out_channel1,\\\n",
    "                               kernel_size=5, stride=5, padding=0)\n",
    "        self.conv1_bn = nn.BatchNorm1d(self.out_channel1)\n",
    "#         dropout_rate_conv1 = trial.suggest_categorical(\"RCM_flanking_dropout_rate_conv1\", [0, 0.1, 0.2, 0.4, 0.8])\n",
    "#         dropout_rate_conv1 = 0\n",
    "#         self.drop_conv1 = nn.Dropout(p=dropout_rate_conv1)\n",
    "        \n",
    "        \n",
    "#         self.out_channel2 = trial.suggest_categorical('RCM_flanking_out_channel2', [32, 64, 128, 256, 512])\n",
    "        self.out_channel2 = 32\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.out_channel1, out_channels=self.out_channel2,\\\n",
    "                               kernel_size=5, stride=5, padding=0)\n",
    "        \n",
    "        self.conv2_bn = nn.BatchNorm1d(self.out_channel2)\n",
    "#         dropout_rate_conv2 = trial.suggest_categorical(\"RCM_flanking_dropout_rate_conv2\", [0, 0.1, 0.2, 0.4, 0.8])\n",
    "#         dropout_rate_conv2 = 0\n",
    "#         self.drop_conv2 = nn.Dropout(p=dropout_rate_conv2)\n",
    "        \n",
    "        \n",
    "        self.conv2_out_dim = 5\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = torch.relu(self.conv1_bn(self.conv1(out)))\n",
    "        out = torch.relu(self.conv2_bn(self.conv2(out)))\n",
    "\n",
    "        out = out.view(-1, self.out_channel2 * self.conv2_out_dim)\n",
    "        return out\n",
    "\n",
    "\n",
    "    \n",
    "class RCM_optuna_upper(nn.Module):\n",
    "    '''\n",
    "        This is for 2-d model to process the RCM score distribution of the upper introns\n",
    "    '''\n",
    "    def __init__(self, trial):\n",
    "        \n",
    "        super(RCM_optuna_upper, self).__init__()\n",
    "        \n",
    "        # convlayer 1\n",
    "#         self.out_channel1 = trial.suggest_categorical('RCM_upper_out_channel1', [32, 64, 128, 256, 512])\n",
    "        self.out_channel1 = 512\n",
    "        kernel_size1 = 5\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=5, out_channels=self.out_channel1,\\\n",
    "                               kernel_size=kernel_size1, stride=5, padding=0)\n",
    "        \n",
    "        self.conv1_bn = nn.BatchNorm1d(self.out_channel1)\n",
    "#         dropout_rate_conv1 = trial.suggest_categorical(\"RCM_upper_dropout_rate_conv1\", [0, 0.1, 0.2, 0.4, 0.8])\n",
    "#         dropout_rate_conv1 = 0\n",
    "#         self.drop_conv1 = nn.Dropout(p=dropout_rate_conv1)\n",
    "        \n",
    "        \n",
    "#         self.out_channel2 = trial.suggest_categorical('RCM_upper_out_channel2', [32, 64, 128, 256, 512])\n",
    "        self.out_channel2 = 64\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.out_channel1, out_channels=self.out_channel2,\\\n",
    "                               kernel_size=5, stride=5, padding=0)\n",
    "        \n",
    "        self.conv2_bn = nn.BatchNorm1d(self.out_channel2)\n",
    "#         dropout_rate_conv2 = trial.suggest_categorical(\"RCM_upper_dropout_rate_conv2\", [0, 0.1, 0.2, 0.4, 0.8])\n",
    "#         dropout_rate_conv2 = 0\n",
    "#         self.drop_conv2 = nn.Dropout(p=dropout_rate_conv2)\n",
    "        \n",
    "        \n",
    "        self.conv2_out_dim = 5\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = torch.relu(self.conv1_bn(self.conv1(out)))\n",
    "        out = torch.relu(self.conv2_bn(self.conv2(out)))\n",
    "\n",
    "        out = out.view(-1, self.out_channel2 * self.conv2_out_dim)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class RCM_optuna_lower(nn.Module):\n",
    "    '''\n",
    "        This is for 2-d model to process the RCM score distribution of the lower introns\n",
    "    '''\n",
    "    def __init__(self, trial):\n",
    "        \n",
    "        super(RCM_optuna_lower, self).__init__()\n",
    "        \n",
    "        # convlayer 1\n",
    "#         self.out_channel1 = trial.suggest_categorical('RCM_lower_out_channel1', [32, 64, 128, 256, 512])\n",
    "        self.out_channel1 = 512\n",
    "        kernel_size1 = 5\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=5, out_channels=self.out_channel1,\\\n",
    "                               kernel_size=kernel_size1, stride=5, padding=0)\n",
    "        \n",
    "        self.conv1_bn = nn.BatchNorm1d(self.out_channel1)\n",
    "#         dropout_rate_conv1 = trial.suggest_categorical(\"RCM_lower_dropout_rate_conv1\", [0, 0.1, 0.2, 0.4, 0.8])\n",
    "#         dropout_rate_conv1 = 0\n",
    "#         self.drop_conv1 = nn.Dropout(p=dropout_rate_conv1)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.out_channel2 = trial.suggest_categorical('RCM_lower_out_channel2', [32, 64, 128, 256, 512])\n",
    "        self.out_channel2 = 512\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.out_channel1, out_channels=self.out_channel2,\\\n",
    "                               kernel_size=5, stride=5, padding=0)\n",
    "        \n",
    "        self.conv2_bn = nn.BatchNorm1d(self.out_channel2)\n",
    "#         dropout_rate_conv2 = trial.suggest_categorical(\"RCM_lower_dropout_rate_conv2\", [0, 0.1, 0.2, 0.4, 0.8])\n",
    "#         dropout_rate_conv2 = 0\n",
    "#         self.drop_conv2 = nn.Dropout(p=dropout_rate_conv2)\n",
    "        self.conv2_out_dim = 5\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = torch.relu(self.conv1_bn(self.conv1(out)))\n",
    "        out = torch.relu(self.conv2_bn(self.conv2(out)))\n",
    "\n",
    "        out = out.view(-1, self.out_channel2 * self.conv2_out_dim)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "\n",
    "class RCM_optuna_concate(nn.Module):\n",
    "    ''''\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, trial):\n",
    "        \n",
    "        super(RCM_optuna_concate, self).__init__()\n",
    "\n",
    "        ### cnn for the flanking rcm scores\n",
    "        self.cnn_flanking = RCM_optuna_flanking(trial)\n",
    "\n",
    "        self.flanking_out_dim = self.cnn_flanking.conv2_out_dim\n",
    "        self.flanking_out_channel = self.cnn_flanking.out_channel2\n",
    "#         print(f'flanking out dim: {self.flanking_out_dim}, flanking out channel {self.flanking_out_channel}')\n",
    "        \n",
    "        ### cnn for the upper rcm scores\n",
    "        self.cnn_upper = RCM_optuna_upper(trial)\n",
    "\n",
    "        self.upper_out_dim = self.cnn_upper.conv2_out_dim\n",
    "        self.upper_out_channel = self.cnn_upper.out_channel2\n",
    "#         print(f'upper_out_dim: {self.upper_out_dim}, upper_out_channel {self.upper_out_channel}')\n",
    "        \n",
    "        ### cnn for the lower rcm scores\n",
    "        self.cnn_lower = RCM_optuna_lower(trial)\n",
    "\n",
    "        self.lower_out_dim = self.cnn_lower.conv2_out_dim\n",
    "        self.lower_out_channel = self.cnn_lower.out_channel2\n",
    "#         print(f'lower_out_dim: {self.lower_out_dim}, lower_out_channel {self.lower_out_channel}')\n",
    "        \n",
    "\n",
    "        self.fc1_input_dim = self.flanking_out_dim * self.flanking_out_channel + \\\n",
    "                             self.upper_out_dim * self.upper_out_channel + \\\n",
    "                             self.lower_out_dim * self.lower_out_channel\n",
    "\n",
    "#         print(f'fc1_input_dim: {self.fc1_input_dim}')\n",
    "        \n",
    "        \n",
    "#         self.fc1_out = trial.suggest_categorical('concat_fc1_out', [32, 64, 128, 256, 512])\n",
    "        self.fc1_out = 256\n",
    "    \n",
    "        # add the rcm feature dimension here as well (5*5+2)*3+2 = 83\n",
    "        self.fc1 = nn.Linear(self.fc1_input_dim, self.fc1_out)\n",
    "        \n",
    "        self.fc1_bn = nn.BatchNorm1d(self.fc1_out)\n",
    "\n",
    "#         dropout_rate_fc1 = trial.suggest_categorical(\"concat_dropout_rate_fc1\",  [0, 0.1, 0.2, 0.4, 0.8])\n",
    "        dropout_rate_fc1 = 0\n",
    "        self.drop_nn1 = nn.Dropout(p=dropout_rate_fc1)\n",
    "\n",
    "        # fc layer2\n",
    "        # use dimension output with nn.CrossEntropyLoss()\n",
    "#         self.fc2_out = trial.suggest_categorical('concat_fc2_out', [8, 16, 32, 64, 128])\n",
    "        self.fc2_out = 64\n",
    "        self.fc2 = nn.Linear(self.fc1_out, self.fc2_out)\n",
    "\n",
    "        self.fc2_bn = nn.BatchNorm1d(self.fc2_out)\n",
    "\n",
    "#         dropout_rate_fc2 = trial.suggest_categorical(\"concat_dropout_rate_fc2\",[0, 0.1, 0.2, 0.4, 0.8])\n",
    "        dropout_rate_fc2 = 0\n",
    "    \n",
    "        self.drop_nn2 = nn.Dropout(p=dropout_rate_fc2)\n",
    "\n",
    "#         self.fc3 = nn.Linear(self.fc2_out, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, rcm_flanking, rcm_upper, rcm_lower):\n",
    "        \n",
    "        x1 = self.cnn_flanking(rcm_flanking)\n",
    "\n",
    "        x2 = self.cnn_upper(rcm_upper)\n",
    "        \n",
    "        x3 = self.cnn_lower(rcm_lower)\n",
    "        \n",
    "        x = torch.cat((x1,x2,x3), dim=1)\n",
    "    \n",
    "        # feed the concatenated feature to fc1\n",
    "        out = self.fc1(x)\n",
    "        out = self.drop_nn1(torch.relu(self.fc1_bn(out)))\n",
    "        out = self.fc2(out)\n",
    "        out = self.drop_nn2(torch.relu(self.fc2_bn(out)))\n",
    "#         out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "        \n",
    "\n",
    "class ConcatModel1_optuna(nn.Module):\n",
    "    ''''\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, trial, cnn_num, include_rcm_tri_cnn):\n",
    "        \n",
    "        self.cnn_num = cnn_num\n",
    "        \n",
    "        ### check whether to include mlp to process the rcm and a2i features\n",
    "        self.include_rcm_tri_cnn = include_rcm_tri_cnn\n",
    "        \n",
    "        super(ConcatModel1_optuna, self).__init__()\n",
    "\n",
    "        ### cnn for the concatenated sequence\n",
    "        self.cnn = Model1_optuna(trial, self.cnn_num)\n",
    "        \n",
    "        if self.cnn_num == 2:\n",
    "            # this is for two convlayer\n",
    "            self.out_dim = self.cnn.conv2_out_dim\n",
    "            self.out_channel = self.cnn.out_channel2\n",
    "        else:\n",
    "            # this is for one convlayer\n",
    "            self.out_dim = self.cnn.conv1_out_dim\n",
    "            self.out_channel = self.cnn.out_channel1\n",
    "        \n",
    "        if self.include_rcm_tri_cnn:\n",
    "            self.rcm_tri_cnn = RCM_optuna_concate(trial)\n",
    "            \n",
    "#             self.rcm_flanking_cnn = RCM_optuna_flanking(trial)\n",
    "#             self.rcm_upper_cnn = RCM_optuna_upper(trial)\n",
    "#             self.rcm_lower_cnn = RCM_optuna_lower(trial)\n",
    "            \n",
    "            self.fc1_input_dim = self.rcm_tri_cnn.fc2_out + self.out_channel * self.out_dim \n",
    "        \n",
    "#             self.fc1_input_dim = self.out_channel * self.out_dim + \\\n",
    "#                                  self.rcm_flanking_cnn.conv2_out_dim * self.rcm_flanking_cnn.out_channel2 + \\\n",
    "#                                  self.rcm_upper_cnn.conv2_out_dim * self.rcm_upper_cnn.out_channel2 + \\\n",
    "#                                  self.rcm_lower_cnn.conv2_out_dim * self.rcm_lower_cnn.out_channel2\n",
    "       \n",
    "            ### just use the junction sequence of singleCNN\n",
    "        else:\n",
    "            self.fc1_input_dim = self.out_channel * self.out_dim\n",
    "            \n",
    "\n",
    "        self.fc1_out = trial.suggest_categorical('concat_fc1_out', [64, 128, 256, 512])\n",
    "#         self.fc1_out = 256\n",
    "    \n",
    "        # add the rcm feature dimension here as well (5*5+2)*3+2 = 83\n",
    "        self.fc1 = nn.Linear(self.fc1_input_dim, self.fc1_out)\n",
    "        \n",
    "        self.fc1_bn = nn.BatchNorm1d(self.fc1_out)\n",
    "\n",
    "        dropout_rate_fc1 = trial.suggest_categorical(\"concat_dropout_rate_fc1\",  [0, 0.1, 0.2, 0.4])\n",
    "#         dropout_rate_fc1 = 0\n",
    "        self.drop_nn1 = nn.Dropout(p=dropout_rate_fc1)\n",
    "\n",
    "        # fc layer2\n",
    "        # use dimension output with nn.CrossEntropyLoss()\n",
    "        self.fc2_out = trial.suggest_categorical('concat_fc2_out', [8, 16, 32, 64])\n",
    "#         self.fc2_out = 64\n",
    "        self.fc2 = nn.Linear(self.fc1_out, self.fc2_out)\n",
    "\n",
    "        self.fc2_bn = nn.BatchNorm1d(self.fc2_out)\n",
    "\n",
    "        dropout_rate_fc2 = trial.suggest_categorical(\"concat_dropout_rate_fc2\",[0, 0.1, 0.2, 0.4])\n",
    "#         dropout_rate_fc2 = 0\n",
    "    \n",
    "        self.drop_nn2 = nn.Dropout(p=dropout_rate_fc2)\n",
    "        \n",
    "#         fc layer 3\n",
    "#         self.fc3_out = trial.suggest_categorical('concat_fc3_out', [4, 8, 16, 32])\n",
    "# # #         self.fc2_out = 64\n",
    "#         self.fc3 = nn.Linear(self.fc2_out, self.fc3_out)\n",
    "\n",
    "#         self.fc3_bn = nn.BatchNorm1d(self.fc3_out)\n",
    "\n",
    "#         dropout_rate_fc3 = trial.suggest_categorical(\"concat_dropout_rate_fc3\",[0, 0.1, 0.2, 0.4])\n",
    "# #         dropout_rate_fc3 = 0\n",
    "    \n",
    "#         self.drop_nn3 = nn.Dropout(p=dropout_rate_fc3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(self.fc2_out, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, seq_upper_lower_feature, rcm_flanking=None, rcm_upper=None, rcm_lower=None):\n",
    "        \n",
    "        x1 = self.cnn(seq_upper_lower_feature)\n",
    "            \n",
    "        if self.include_rcm_tri_cnn:\n",
    "            x2 = self.rcm_tri_cnn(rcm_flanking, rcm_upper, rcm_lower)\n",
    "            x = torch.cat((x1, x2), dim=1)\n",
    "            ### normalization after concatenation\n",
    "            x = torch.nn.functional.normalize(x)\n",
    "#             x2 = self.rcm_flanking_cnn(rcm_flanking)\n",
    "#             x3 = self.rcm_upper_cnn(rcm_upper)\n",
    "#             x4 = self.rcm_lower_cnn(rcm_lower)\n",
    "#             x = torch.cat((x1,x2,x3,x4), dim=1)\n",
    "        else:\n",
    "            x = x1\n",
    "    \n",
    "        # feed the concatenated feature to fc1\n",
    "        out = self.fc1(x)\n",
    "        out = self.drop_nn1(torch.relu(self.fc1_bn(out)))\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.drop_nn2(torch.relu(self.fc2_bn(out)))\n",
    "        \n",
    "#         out = self.fc3(out)\n",
    "#         out = self.drop_nn3(torch.relu(self.fc3_bn(out)))\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2750490b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aef586cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Objective(device, trial, fold, model, optimizer,\n",
    "              patience, epochs, criterion, train_loader, \n",
    "              val_loader, model_folder):\n",
    "#     print(f\"I'm in the fold: {fold}\")\n",
    "#     print('here is my model structure', model)\n",
    "    ### implement the early stopping based on the validation loss change\n",
    "    last_val_loss = 1000 # set to some big number\n",
    "    counter = 0 # count the patience so far\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "#         print(f\"I'am in the epoch {epoch}\")\n",
    "        model.train()\n",
    "        # record the training loss\n",
    "        running_loss = 0.0\n",
    "\n",
    "        ## deal with different number of features in different dataset with star* notation\n",
    "        for *features, train_labels in train_loader:\n",
    "            ### this line is just for nn.CrossEntropy loss otherwise can be safely removed\n",
    "            train_labels = train_labels.type(torch.LongTensor)\n",
    "\n",
    "            train_labels = train_labels.to(device)\n",
    "            features = [i.to(device) for i in features]\n",
    "\n",
    "            # forward pass\n",
    "            train_preds = model(*features)\n",
    "            loss = criterion(train_preds, train_labels)\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()  # empty the gradient from last round\n",
    "\n",
    "            # calculate the gradient\n",
    "            loss.backward()\n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "#         print(f\"I'am finished the epoch {epoch} training\")\n",
    "        ## start model validation\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ## first evaluate the training acc\n",
    "            correct, total = 0.0, 0.0\n",
    "            for *features, train_labels in train_loader:\n",
    "                ### this type conversion is just used for nn.CrossEntropy loss\n",
    "                ### otherwise can be safely removed\n",
    "                train_labels = train_labels.to(device)\n",
    "                features = [i.to(device) for i in features]\n",
    "\n",
    "                # get the predition with the model parameters updated after each epoch\n",
    "                preds = model(*features)\n",
    "\n",
    "                # prediction for the nn.CrossEntropy loss\n",
    "                _, preds_labels = torch.max(preds, 1)\n",
    "                correct += (preds_labels == train_labels).sum().item()\n",
    "                total += train_labels.shape[0]\n",
    "\n",
    "            train_acc = round(correct / total, 4)\n",
    "            \n",
    "#             print(f\"I'am finished the epoch {epoch} evaluation on the training set\")\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f'fold {fold + 1}, epoch {epoch + 1}, training loss {running_loss}, train accuracy {train_acc}')\n",
    "\n",
    "            # evaluate the validation accuracy and other metrics after each epoch\n",
    "            correct, total = 0.0, 0.0\n",
    "            val_loss_list = []\n",
    "            # store the validation prediction and validation label for different metric calculation\n",
    "            val_labels_list = []\n",
    "            val_preds_list = []\n",
    "\n",
    "            for *val_features, val_labels in val_loader:\n",
    "                ### this type conversion is just used for nn.CrossEntropy loss\n",
    "                ### otherwise can be safely removed\n",
    "                val_labels = val_labels.type(torch.LongTensor)\n",
    "\n",
    "                val_labels = val_labels.to(device)\n",
    "                val_features = [i.to(device) for i in val_features]\n",
    "\n",
    "                # get the predition with the model parameters updated after each epoch\n",
    "                val_preds = model(*val_features)\n",
    "\n",
    "                # get the validation loss for the early stopping\n",
    "                val_loss = criterion(val_preds, val_labels)\n",
    "                val_loss_list.append(val_loss.item())\n",
    "\n",
    "                # prediction for the nn.CrossEntropy loss\n",
    "                _, preds_labels = torch.max(val_preds, 1)\n",
    "\n",
    "                ## append the true and predict value in lists for the calculation of \n",
    "                ## model performance\n",
    "                val_preds_list.append(preds_labels)\n",
    "                val_labels_list.append(val_labels)\n",
    "\n",
    "                correct += (preds_labels == val_labels).sum().item()\n",
    "                total += val_labels.shape[0]\n",
    "                \n",
    "            val_acc = round(correct / total, 4)\n",
    "            \n",
    "#             print(f\"I'am finished the epoch {epoch} evaluation on the validation set\")\n",
    "            \n",
    "            total_val_loss = np.sum(val_loss_list)\n",
    "            \n",
    "            val_labels_total = torch.cat(val_labels_list, dim=0)\n",
    "            val_preds_total = torch.cat(val_preds_list, dim=0)\n",
    "            ## calculate the f1 score\n",
    "#                     f1_score = f1(val_preds_total, val_labels_total)\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f'fold {fold + 1}, epoch {epoch + 1}, val accuracy {val_acc}')\n",
    "\n",
    "        ### early stopping checking and save the model after each epoch if the trial is not pruned\n",
    "        if total_val_loss <= last_val_loss:\n",
    "#             print(f\"the total val loss is {total_val_loss} on epoch {epoch}\")\n",
    "            last_val_loss = total_val_loss\n",
    "            best_val_acc = val_acc # save the best val_acc so far\n",
    "#                 print(f'epoch {epoch+1} total val loss:{total_val_loss}')\n",
    "\n",
    "            \n",
    "#                 print(f'epoch {epoch + 1}, val loss {total_val_loss}, val accuracy {val_acc}')\n",
    "            ## set counter to 0 to start checking if the next 10 consecutive epoches are having reduced val loss\n",
    "            counter = 0\n",
    "\n",
    "            ## this line will overwrite the model and save the best one in each fold\n",
    "            ## with the lowest val loss in each trial\n",
    "            model_path = f\"{model_folder}/fold{fold+1}_trial{trial.number}.pt\"\n",
    "            torch.save(model, model_path)\n",
    "\n",
    "        else:\n",
    "#                 print(f'epoch {epoch+1} total val loss:{total_val_loss}')\n",
    "            counter += 1\n",
    "#                 print(counter)\n",
    "            if counter >= patience:\n",
    "                break # break out of the epoch loop and into the next fold\n",
    "            \n",
    "    return best_val_acc ## best validation accuracy in each fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87651640",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective_CV:\n",
    "    \n",
    "    def __init__(self, patience, cv, model, dataset, cnn_num, is_simple_cnn,\n",
    "                 is_bicnn, include_rcm_tri_cnn, val_acc_folder, model_folder):\n",
    "        \n",
    "        self.patience = patience ## number of epochs for early stopping the model training\n",
    "        self.cv = cv ## number of CV\n",
    "        self.model = model ## pass the corresponding model\n",
    "        self.dataset = dataset ## the corresponding dataset object \n",
    "        \n",
    "        self.cnn_num = cnn_num ## number of CNN layers for either simple_cnn or bi_cnn\n",
    "        \n",
    "        ### check which model to initiate\n",
    "        self.is_simple_cnn = is_simple_cnn ## model that just concatenate junction sequences\n",
    "        self.is_bicnn = is_bicnn ## model that use bi CNN\n",
    "        \n",
    "        self.include_rcm_tri_cnn =  include_rcm_tri_cnn ## whether include rcm_cnn in the CNN\n",
    "        \n",
    "        self.val_acc_folder = val_acc_folder ## folder to store the cross_validation accuracy \n",
    "        self.model_folder = model_folder ## folder to store the trained model for later testing dataset evaluation\n",
    "        \n",
    "    def __call__(self, trial):\n",
    "        \n",
    "             ### just use the sequence feature for now\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "#         lr = 0.0003\n",
    "        l2_lambda = trial.suggest_float(\"l2_lambda\", 1e-8, 1e-3, log=True)\n",
    "#         l2_lambda = 0\n",
    "\n",
    "\n",
    "        ### fix and use the maximal allowed batch size\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [128, 256, 512, 1024])\n",
    "#         batch_size = 512\n",
    "\n",
    "        ### optimize epoch number\n",
    "        epochs = 150\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        kfold = KFold(n_splits=self.cv, shuffle=True)\n",
    "        \n",
    "        val_acc_list = []\n",
    "        \n",
    "        for fold, (train_index, val_index) in enumerate(kfold.split(np.arange(len(self.dataset)))):\n",
    "            \n",
    "            ### get the train and val loader\n",
    "            train_subset = torch.utils.data.Subset(self.dataset, train_index)\n",
    "            train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "            val_subset = torch.utils.data.Subset(self.dataset, val_index)\n",
    "            val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "            \n",
    "            ## model should be initilized here for each fold to have a new model with same hyperparameters\n",
    "            \n",
    "            ### for the model the process the concatenated upper and lower the is_rcm is always False\n",
    "            if self.is_simple_cnn: \n",
    "                ### create model for simple cnn and include or exclude rcm and a2i mlp as subnetworks\n",
    "                model = self.model(trial, self.cnn_num, self.include_rcm_tri_cnn).to(device=device)\n",
    "#                 print(model)\n",
    "            elif self.is_bicnn:\n",
    "                ### create model for biCNN and include or exclude rcm and a2i mlp as subnetworks\n",
    "                model = self.model(trial, self.cnn_num, self.include_rcm_tri_cnn).to(device=device)\n",
    "#                 print(model)\n",
    "                \n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_lambda)\n",
    "            \n",
    "            accuracy = Objective(device, trial, fold=fold, model=model, optimizer=optimizer,\n",
    "                                 patience=self.patience, epochs=epochs, criterion=criterion, \n",
    "                                 train_loader=train_loader, val_loader=val_loader, \n",
    "                                 model_folder=self.model_folder)\n",
    "            \n",
    "            val_acc_list.append(accuracy)\n",
    "            \n",
    "        ### to speed up training for cv just maximize the val_acc from the 3 fold cv\n",
    "       ## choose the best model structure and hyperparameters based on average 3-cv validation acc\n",
    "    \n",
    "        avg_acc_val = np.mean(val_acc_list)\n",
    "        \n",
    "        val_acc_path = f\"{self.val_acc_folder}/val_acc.csv\"\n",
    "        \n",
    "        val_acc_str = '\\t'.join([str(i) for i in val_acc_list])\n",
    "        with open(val_acc_path, 'a') as f:\n",
    "            f.write('trial' + str(trial.number) + '\\t' + val_acc_str + '\\n')\n",
    "            \n",
    "        return avg_acc_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "899da1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BS_LS_coordinates_path = '/home/wangc90/circRNA/circRNA_Data/BS_LS_data/updated_data/BS_LS_coordinates_final.csv'\n",
    "hg19_seq_dict_json_path = '/home/wangc90/circRNA/circRNA_Data/hg19_seq/hg19_seq_dict.json'\n",
    "flanking_dict_folder = '/home/wangc90/circRNA/circRNA_Data/BS_LS_data/flanking_dicts/'\n",
    "bs_ls_dataset = BS_LS_DataSet_Prep(BS_LS_coordinates_path=BS_LS_coordinates_path,\n",
    "                                   hg19_seq_dict_json_path=hg19_seq_dict_json_path,\n",
    "                                   flanking_dict_folder=flanking_dict_folder,\n",
    "                                   flanking_junction_bps=100,\n",
    "                                   flanking_intron_bps=4500,\n",
    "                                   training_size=11000)\n",
    "\n",
    "\n",
    "### generate the junction and flanking intron dict\n",
    "bs_ls_dataset.get_junction_flanking_intron_seq()\n",
    "\n",
    "### used the same training keys for RCM and A2I mlp structure selection\n",
    "train_keys, test_keys = bs_ls_dataset.get_train_test_keys()\n",
    "\n",
    "rcm_scores_folder = '/home/wangc90/circRNA/circRNA_Data/BS_LS_data/flanking_dicts/rcm_scores/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51f8e20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25942"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_keys)+ len(test_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e30fa0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['chr15|55669146|55759359|-', 'chr5|130977406|131006324|-',\n",
       "       'chr16|88037900|88098938|+', ..., 'chr4|41526425|41668683|+',\n",
       "       'chr4|75066979|75147267|+', 'chr2|56419611|56599613|+'],\n",
       "      dtype='<U27')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f85d72eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_torch_upper_lower_features,\\\n",
    "train_torch_rcm_flanking_features, train_torch_rcm_upper_features,\\\n",
    "train_torch_rcm_lower_features, train_torch_labels = bs_ls_dataset.seq_to_tensor(data_keys=train_keys, rcm_folder=rcm_scores_folder, is_rcm=True, is_upper_lower_concat=True)\n",
    "\n",
    "\n",
    "BS_LS_dataset = BS_LS_upper_lower_concat_rcm(include_rcm=True,\n",
    "                                             seq_upper_lower_feature=train_torch_upper_lower_features,\n",
    "                                             flanking_rcm=train_torch_rcm_flanking_features,\n",
    "                                             upper_rcm=train_torch_rcm_upper_features,\n",
    "                                             lower_rcm=train_torch_rcm_lower_features,\n",
    "                                             label=train_torch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3f283a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_model1_selection_optuna(num_trial, train_keys):\n",
    "\n",
    "    ### where to save the 3-fold CV validation acc\n",
    "\n",
    "    val_acc_folder = '/home/wangc90/circRNA/circRNA_Data/model_outputs/combined_model1/val_acc_cv3'\n",
    "    ### where to save the best model in the 3-fold CV \n",
    "    model_folder = '/home/wangc90/circRNA/circRNA_Data/model_outputs/combined_model1/models'\n",
    "    ### wehre to save the detailed optuna results\n",
    "    optuna_folder = '/home/wangc90/circRNA/circRNA_Data/model_outputs/combined_model1/optuna'\n",
    "    \n",
    "    ## try without rcm features\n",
    "#     train_torch_upper_lower_features, train_torch_labels = bs_ls_dataset.seq_to_tensor(data_keys=train_keys, rcm_folder=rcm_scores_folder, is_rcm=False, is_upper_lower_concat=True)\n",
    "    \n",
    "#     BS_LS_dataset = BS_LS_upper_lower_concat_rcm(include_rcm=False,\n",
    "#                                           seq_upper_lower_feature=train_torch_upper_lower_features,\n",
    "#                                           flanking_rcm=None,\n",
    "#                                           upper_rcm=None,\n",
    "#                                           lower_rcm=None,\n",
    "#                                           label=train_torch_labels)\n",
    "    \n",
    "    train_torch_upper_lower_features,\\\n",
    "    train_torch_rcm_flanking_features, train_torch_rcm_upper_features,\\\n",
    "    train_torch_rcm_lower_features, train_torch_labels = bs_ls_dataset.seq_to_tensor(data_keys=train_keys, rcm_folder=rcm_scores_folder, is_rcm=True, is_upper_lower_concat=True)\n",
    "\n",
    "    \n",
    "    BS_LS_dataset = BS_LS_upper_lower_concat_rcm(include_rcm=True,\n",
    "                                                 seq_upper_lower_feature=train_torch_upper_lower_features,\n",
    "                                                 flanking_rcm=train_torch_rcm_flanking_features,\n",
    "                                                 upper_rcm=train_torch_rcm_upper_features,\n",
    "                                                 lower_rcm=train_torch_rcm_lower_features,\n",
    "                                                 label=train_torch_labels)\n",
    "\n",
    "#     print(len(BS_LS_dataset))\n",
    "#     print(BS_LS_dataset[0][0].shape, BS_LS_dataset[0][1].shape, BS_LS_dataset[0][2].shape)\n",
    "    \n",
    "    study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=2),\n",
    "                                direction='maximize')\n",
    "    \n",
    "    study.optimize(Objective_CV(patience=10, cv=3, model= ConcatModel1_optuna, \n",
    "                            dataset=BS_LS_dataset,\n",
    "                            cnn_num=2, is_simple_cnn=True,\n",
    "                            is_bicnn=False, include_rcm_tri_cnn=True,\n",
    "                            val_acc_folder=val_acc_folder,\n",
    "                            model_folder=model_folder), n_trials=num_trial, gc_after_trial=True)\n",
    "\n",
    "\n",
    "    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "    complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "    with open(optuna_folder+'/optuna.txt', 'a') as f:\n",
    "        f.write(\"Study statistics: \\n\")\n",
    "        f.write(f\"Number of finished trials: {len(study.trials)}\\n\")\n",
    "        f.write(f\"Number of pruned trials: {len(pruned_trials)}\\n\")\n",
    "        f.write(f\"Number of complete trials: {len(complete_trials)}\\n\")\n",
    "\n",
    "        f.write(\"Best trial:\\n\")\n",
    "        trial = study.best_trial\n",
    "        f.write(f\"Value: {trial.value}\\n\")\n",
    "        f.write(\"Params:\\n\")\n",
    "        for key, value in trial.params.items():\n",
    "            f.write(f\"{key}:{value}\\n\")\n",
    "\n",
    "    df = study.trials_dataframe().drop(['state','datetime_start','datetime_complete','duration','number'], axis=1)\n",
    "    df.to_csv(optuna_folder + '/optuna.csv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d13df8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-24 20:10:04,177]\u001b[0m A new study created in memory with name: no-name-234debaf-f64e-455f-a890-00c8eafa1bda\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1, epoch 20, training loss 13.18324564397335, train accuracy 0.9999\n",
      "fold 1, epoch 20, val accuracy 0.8206\n",
      "fold 1, epoch 40, training loss 8.222176551818848, train accuracy 1.0\n",
      "fold 1, epoch 40, val accuracy 0.8292\n",
      "fold 1, epoch 60, training loss 5.735739767551422, train accuracy 1.0\n",
      "fold 1, epoch 60, val accuracy 0.8383\n",
      "fold 1, epoch 80, training loss 4.022277180105448, train accuracy 1.0\n",
      "fold 1, epoch 80, val accuracy 0.8395\n",
      "fold 1, epoch 100, training loss 2.8500091172754765, train accuracy 1.0\n",
      "fold 1, epoch 100, val accuracy 0.8377\n",
      "fold 2, epoch 20, training loss 7.838870480656624, train accuracy 1.0\n",
      "fold 2, epoch 20, val accuracy 0.8309\n",
      "fold 2, epoch 40, training loss 4.542447112500668, train accuracy 1.0\n",
      "fold 2, epoch 40, val accuracy 0.8346\n",
      "fold 2, epoch 60, training loss 3.0808320567011833, train accuracy 1.0\n",
      "fold 2, epoch 60, val accuracy 0.8364\n",
      "fold 3, epoch 20, training loss 10.071493342518806, train accuracy 0.9997\n",
      "fold 3, epoch 20, val accuracy 0.8361\n",
      "fold 3, epoch 40, training loss 6.065846487879753, train accuracy 1.0\n",
      "fold 3, epoch 40, val accuracy 0.8392\n",
      "fold 3, epoch 60, training loss 4.180355742573738, train accuracy 1.0\n",
      "fold 3, epoch 60, val accuracy 0.8413\n",
      "fold 3, epoch 80, training loss 2.9887645058333874, train accuracy 1.0\n",
      "fold 3, epoch 80, val accuracy 0.8406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-24 21:20:49,511]\u001b[0m Trial 0 finished with value: 0.839 and parameters: {'lr': 1.0854792676433357e-05, 'l2_lambda': 3.944333673705097e-05, 'batch_size': 256, 'concat_fc1_out': 512, 'concat_dropout_rate_fc1': 0.2, 'concat_fc2_out': 32, 'concat_dropout_rate_fc2': 0.1}. Best is trial 0 with value: 0.839.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "combined_model1_selection_optuna(1, train_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe0343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a82408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
