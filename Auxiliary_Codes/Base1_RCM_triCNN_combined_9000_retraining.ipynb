{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e453af",
   "metadata": {},
   "source": [
    "### Combine the Base model 1 with the RCM_triCNN_all on the remaining 2000 exon pairs\n",
    "#### batch_size, lr, l2, epoch, as well as additional FC layers and associated dropout layers need to be optimized on this stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e14640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import combinations\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "import optuna\n",
    "from torchmetrics.classification import F1Score\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "### import Dataset prepartion and model training classes from BS_LS_scripts folder\n",
    "sys.path.insert(1, '/home/wangc90/circRNA/circRNA_Data/BS_LS_scripts/')\n",
    "from BS_LS_DataSet_3 import BS_LS_DataSet_Prep, BS_LS_upper_lower_concat_rcm\n",
    "# from BS_LS_Training_Base_models_1 import Objective, Objective_CV\n",
    "random.seed(2023)\n",
    "torch.manual_seed(2023)\n",
    "from pre_trained_model_structure import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af661f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### get the model structure to flatten layers first\n",
    "\n",
    "class Model1_optuna_9000(nn.Module):\n",
    "    '''\n",
    "        This model take in input sequence 4 X 400 with 1 CNN layer\n",
    "    '''\n",
    "\n",
    "    def __init__(self, trial):\n",
    "\n",
    "        super(Model1_optuna_9000, self).__init__()\n",
    "        ### first CNN layer\n",
    "#         self.out_channel1 = trial.suggest_categorical('out_channel1', [128, 256, 512])\n",
    "        self.out_channel1 = 512\n",
    "#         kernel_size1 = trial.suggest_categorical('kernel_size1', [13, 15, 17, 19, 21])\n",
    "        kernel_size1 = 13\n",
    "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=self.out_channel1, \\\n",
    "                               kernel_size=kernel_size1, stride=1, padding=(kernel_size1 - 1) // 2)\n",
    "        self.conv1_bn = nn.BatchNorm1d(self.out_channel1)\n",
    "#         self.maxpool1 = trial.suggest_categorical('maxpool1', [5, 10, 20])\n",
    "        self.maxpool1 = 5\n",
    "        self.conv1_out_dim = 400 // self.maxpool1\n",
    "\n",
    "#         self.out_channel2 = trial.suggest_categorical('out_channel2', [128, 256, 512])\n",
    "        self.out_channel2 = 512\n",
    "#         kernel_size2 = trial.suggest_categorical('kernel_size2', [13, 15, 17, 19, 21])\n",
    "        kernel_size2 = 21\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.out_channel1, out_channels=self.out_channel2, \\\n",
    "                                   kernel_size=kernel_size2, stride=1, padding=(kernel_size2 - 1) // 2)\n",
    "        self.conv2_bn = nn.BatchNorm1d(self.out_channel2)\n",
    "#         self.maxpool2 = trial.suggest_categorical('maxpool2', [5, 10, 20])\n",
    "        self.maxpool2 = 5\n",
    "        self.conv2_out_dim = 400 // (self.maxpool1 * self.maxpool2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = torch.relu(self.conv1_bn(self.conv1(out)))\n",
    "        out = F.max_pool1d(out, self.maxpool1)\n",
    "        out = torch.relu(self.conv2_bn(self.conv2(out)))\n",
    "        out = F.max_pool1d(out, self.maxpool2)\n",
    "        out = out.view(-1, self.out_channel2 * self.conv2_out_dim)\n",
    "        return out\n",
    "\n",
    "        \n",
    "class RCM_optuna_flanking_9000(nn.Module):\n",
    "    '''\n",
    "        This is for 2-d model to process the RCM score distribution of the flanking introns\n",
    "    '''\n",
    "    def __init__(self, trial):\n",
    "        \n",
    "        super(RCM_optuna_flanking_9000, self).__init__()\n",
    "        \n",
    "        # convlayer 1\n",
    "#         self.out_channel1 = trial.suggest_categorical('flanking_out_channel1', [128, 256, 512])\n",
    "        self.out_channel1 = 512\n",
    "\n",
    "#         kernel_size1 = 5\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=5, out_channels=self.out_channel1,\\\n",
    "                               kernel_size=5, stride=5, padding=0)\n",
    "        self.conv1_bn = nn.BatchNorm1d(self.out_channel1)\n",
    "        \n",
    "#         self.out_channel2 = trial.suggest_categorical('flanking_out_channel2', [128, 256, 512])\n",
    "        self.out_channel2 = 128\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=self.out_channel1, out_channels=self.out_channel2,\\\n",
    "                               kernel_size=5, stride=5, padding=0)\n",
    "        \n",
    "        self.conv2_bn = nn.BatchNorm1d(self.out_channel2)\n",
    "        \n",
    "        self.conv2_out_dim = 10\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = torch.relu(self.conv1_bn(self.conv1(out)))\n",
    "        out = torch.relu(self.conv2_bn(self.conv2(out)))\n",
    "\n",
    "        out = out.view(-1, self.out_channel2 * self.conv2_out_dim)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class RCM_optuna_upper_9000(nn.Module):\n",
    "    '''\n",
    "        This is for 2-d model to process the RCM score distribution of the upper introns\n",
    "    '''\n",
    "    def __init__(self, trial):\n",
    "        \n",
    "        super(RCM_optuna_upper_9000, self).__init__()\n",
    "        \n",
    "        # convlayer 1\n",
    "#         self.out_channel1 = trial.suggest_categorical('upper_out_channel1', [128, 256, 512])\n",
    "        self.out_channel1 = 256\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=5, out_channels=self.out_channel1,\\\n",
    "                               kernel_size=5, stride=5, padding=0)\n",
    "        \n",
    "        self.conv1_bn = nn.BatchNorm1d(self.out_channel1)\n",
    "        \n",
    "#         self.out_channel2 = trial.suggest_categorical('upper_out_channel2', [128, 256, 512])\n",
    "        self.out_channel2 = 256\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=self.out_channel1, out_channels=self.out_channel2,\\\n",
    "                               kernel_size=5, stride=5, padding=0)\n",
    "        \n",
    "        self.conv2_bn = nn.BatchNorm1d(self.out_channel2)\n",
    "        self.conv2_out_dim = 10\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = torch.relu(self.conv1_bn(self.conv1(out)))\n",
    "        out = torch.relu(self.conv2_bn(self.conv2(out)))\n",
    "\n",
    "        out = out.view(-1, self.out_channel2 * self.conv2_out_dim)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class RCM_optuna_lower_9000(nn.Module):\n",
    "    '''\n",
    "        This is for 2-d model to process the RCM score distribution of the lower introns\n",
    "    '''\n",
    "    def __init__(self, trial):\n",
    "        \n",
    "        super(RCM_optuna_lower_9000, self).__init__()\n",
    "        \n",
    "        # convlayer 1\n",
    "#         self.out_channel1 = trial.suggest_categorical('lower_out_channel1', [128, 256, 512])\n",
    "        self.out_channel1 = 128\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=5, out_channels=self.out_channel1,\\\n",
    "                               kernel_size=5, stride=5, padding=0)\n",
    "        \n",
    "        self.conv1_bn = nn.BatchNorm1d(self.out_channel1)\n",
    "#         self.out_channel2 = trial.suggest_categorical('lower_out_channel2', [128, 256, 512])\n",
    "        self.out_channel2 = 512\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=self.out_channel1, out_channels=self.out_channel2,\\\n",
    "                               kernel_size=5, stride=5, padding=0)\n",
    "        \n",
    "        self.conv2_bn = nn.BatchNorm1d(self.out_channel2)\n",
    "        self.conv2_out_dim = 10\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = torch.relu(self.conv1_bn(self.conv1(out)))\n",
    "        out = torch.relu(self.conv2_bn(self.conv2(out)))\n",
    "\n",
    "        out = out.view(-1, self.out_channel2 * self.conv2_out_dim)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class Base1_RCM_triCNN_combined_9000(nn.Module):\n",
    "\n",
    "    def __init__(self, trial):\n",
    "        \n",
    "        super(Base1_RCM_triCNN_combined_9000, self).__init__()\n",
    "\n",
    "        ### cnn for model1\n",
    "        \n",
    "        self.cnn = Model1_optuna_9000(trial)\n",
    "        \n",
    "        ### cnn for RCM features\n",
    "        self.rcm_flanking = RCM_optuna_flanking_9000(trial)\n",
    "        self.rcm_upper = RCM_optuna_upper_9000(trial)\n",
    "        self.rcm_lower = RCM_optuna_lower_9000(trial)\n",
    "        \n",
    "        ### fc1 layers from the ConcatModel1_optuna_9000\n",
    "        \n",
    "        self.fc1_input_dim = self.cnn.conv2_out_dim * self.cnn.out_channel2\n",
    "            \n",
    "        self.fc1_out = 512\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.fc1_input_dim, self.fc1_out)\n",
    "        \n",
    "        self.fc1_bn = nn.BatchNorm1d(self.fc1_out)\n",
    "\n",
    "        dropout_rate_fc1 = 0.2\n",
    "        self.drop_nn1 = nn.Dropout(p=dropout_rate_fc1)\n",
    "\n",
    "        \n",
    "        # fc2 layers from the ConcatModel1_optuna_8000\n",
    "        \n",
    "        self.fc2_out = 16\n",
    "        self.fc2 = nn.Linear(self.fc1_out, self.fc2_out)\n",
    "\n",
    "        self.fc2_bn = nn.BatchNorm1d(self.fc2_out)\n",
    "        \n",
    "        dropout_rate_fc2 = 0\n",
    "        self.drop_nn2 = nn.Dropout(p=dropout_rate_fc2)\n",
    "        \n",
    "        \n",
    "\n",
    "        ### fc1 layers from the RCM_optuna_concate_8000\n",
    "        self.rcm_concate_fc1_out = 512\n",
    "        self.rcm_concate_fc1_in = self.rcm_flanking.conv2_out_dim * self.rcm_flanking.out_channel2 +\\\n",
    "                             self.rcm_upper.conv2_out_dim * self.rcm_upper.out_channel2 +\\\n",
    "                             self.rcm_lower.conv2_out_dim * self.rcm_lower.out_channel2\n",
    "        \n",
    "        self.rcm_concate_fc1 = nn.Linear(self.rcm_concate_fc1_in, self.rcm_concate_fc1_out)\n",
    "\n",
    "        self.rcm_concate_fc1_bn = nn.BatchNorm1d(self.rcm_concate_fc1_out)\n",
    "\n",
    "        dropout_rate_rcm_concate_fc1 = 0.1\n",
    "        self.drop_rcm_concate_fc1 = nn.Dropout(p=dropout_rate_rcm_concate_fc1)\n",
    "        \n",
    "         ### fc2 layers from the RCM_optuna_concate_8000\n",
    "        self.rcm_concate_fc2_out = 32\n",
    "        self.rcm_concate_fc2 = nn.Linear(self.rcm_concate_fc1_out, self.rcm_concate_fc2_out)\n",
    "\n",
    "        self.rcm_concate_fc2_bn = nn.BatchNorm1d(self.rcm_concate_fc2_out)\n",
    "\n",
    "        dropout_rate_rcm_concate_fc2 = 0\n",
    "        self.drop_rcm_concate_fc2 = nn.Dropout(p=dropout_rate_rcm_concate_fc2)\n",
    "        \n",
    "        \n",
    "        ## newly added two layers \n",
    "        \n",
    "#         self.fc3_out = trial.suggest_categorical('Base1_RCM_triCNN_combined_9000_fc3_out',[8, 16, 32])\n",
    "        self.fc3_out = 16\n",
    "        self.fc3 = nn.Linear(self.rcm_concate_fc2_out + self.fc2_out, self.fc3_out)\n",
    "        self.fc3_bn = nn.BatchNorm1d(self.fc3_out)\n",
    "#         dropout_rate_fc3 = trial.suggest_categorical(\"Base1_RCM_triCNN_combined_9000_dropout_rate_fc3\", [0, 0.1, 0.2, 0.4])\n",
    "        dropout_rate_fc3 = 0.4\n",
    "        self.drop_fc3 = nn.Dropout(p=dropout_rate_fc3)\n",
    "        \n",
    "        \n",
    "#         self.fc4_out = trial.suggest_categorical('Base1_RCM_triCNN_combined_9000_fc4_out', [4, 8, 16])\n",
    "        self.fc4_out = 4\n",
    "        self.fc4 = nn.Linear(self.fc3_out, self.fc4_out)\n",
    "        self.fc4_bn = nn.BatchNorm1d(self.fc4_out)\n",
    "        dropout_rate_fc4 = 0.1\n",
    "#         dropout_rate_fc4 = trial.suggest_categorical(\"Base1_RCM_triCNN_combined_9000_dropout_rate_fc4\", [0, 0.1, 0.2, 0.4])\n",
    "        self.drop_fc4 = nn.Dropout(p=dropout_rate_fc4)\n",
    "        \n",
    "        self.fc5_out = 2\n",
    "        self.fc5 = nn.Linear(self.fc4_out, self.fc5_out)\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, seq_upper_lower_feature, rcm_flanking, rcm_upper, rcm_lower):\n",
    "        \n",
    "        x1 = self.cnn(seq_upper_lower_feature)\n",
    "        ### layer to process junction seq\n",
    "        x1 = self.fc1(x1)\n",
    "#         print(x_12.shape)\n",
    "        x1 = self.drop_nn1(torch.relu(self.fc1_bn(x1)))\n",
    "        x1 = self.fc2(x1)\n",
    "        x1 = self.drop_nn2(torch.relu(self.fc2_bn(x1)))\n",
    "        \n",
    "        \n",
    "        ### layer to process RCM information\n",
    "        x2 = self.rcm_flanking(rcm_flanking)\n",
    "        x3 = self.rcm_upper(rcm_upper)\n",
    "        x4 = self.rcm_lower(rcm_lower)\n",
    "        \n",
    "        x_234 = torch.cat((x2,x3,x4), dim=1)\n",
    "        x_234 = self.rcm_concate_fc1(x_234)\n",
    "        \n",
    "        x_234 = self.drop_rcm_concate_fc1(torch.relu(self.rcm_concate_fc1_bn(x_234)))\n",
    "        \n",
    "        x_234 = self.rcm_concate_fc2(x_234)\n",
    "        x_234 = self.drop_rcm_concate_fc2(torch.relu(self.rcm_concate_fc2_bn(x_234)))\n",
    "        \n",
    "        \n",
    "        x_1234 = F.normalize(torch.cat((x1, x_234), dim=1), dim=1)\n",
    "\n",
    "        out = self.drop_fc3(torch.relu(self.fc3_bn(self.fc3(x_1234))))\n",
    "        out = self.drop_fc4(torch.relu(self.fc4_bn(self.fc4(out))))\n",
    "        \n",
    "        out = self.fc5(out)\n",
    "\n",
    "        return out\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e68e4e",
   "metadata": {},
   "source": [
    "#### bring in the model weights for optimized base model 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c3092b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model1_path = \"/home/wangc90/circRNA/circRNA_Data/model_outputs/Base_model1_retraining/Base_model1_retraining_9000/retrained_model_149.pt\"\n",
    "base_model1 = torch.load(base_model1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d37977",
   "metadata": {},
   "source": [
    "#### bring in the model weights for optimized RCM_triCNN_all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72934011",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the new best rcm models !!!\n",
    "RCM_TriCNN_model_path = \"/home/wangc90/circRNA/circRNA_Data/model_outputs/RCM_triCNN_retraining/RCM_triCNN_retraining_9000/retrained_model_29.pt\"\n",
    "RCM_TriCNN_model = torch.load(RCM_TriCNN_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08d2a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retraining(model, dataset, model_folder):\n",
    "    \n",
    "    device = torch.device('cuda:1') if torch. cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    batch_size = 64\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "#     print(len(train_loader))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model = model('trial')\n",
    "#     print(model)\n",
    "\n",
    "    pretrained_dict_upper_lower_concate_cnns = {k: v for k, v in base_model1.state_dict().items() if\n",
    "                            not 'fc3' in k}\n",
    "\n",
    "    pretrained_dict_rcm_cnns = {k: v for k, v in RCM_TriCNN_model.state_dict().items() if not 'fc3' in k}\n",
    "\n",
    "    model_dict = model.state_dict()\n",
    "    #             print('This is the original model weights')\n",
    "    #             print(model_dict)\n",
    "    model_dict.update(pretrained_dict_upper_lower_concate_cnns)\n",
    "    model_dict.update(pretrained_dict_rcm_cnns)\n",
    "\n",
    "\n",
    "    print('Loading the trained model weights to the Base1_RCM_triCNN_combined_9000 model')\n",
    "\n",
    "    model.load_state_dict(model_dict)\n",
    "     ### freeze these parameters\n",
    "    model.cnn.requires_grad_(False)\n",
    "\n",
    "    model.rcm_flanking.requires_grad_(False)\n",
    "    model.rcm_upper.requires_grad_(False)\n",
    "    model.rcm_lower.requires_grad_(False)\n",
    "\n",
    "\n",
    "    model.fc1.requires_grad_(False)\n",
    "    model.fc1_bn.requires_grad_(False)\n",
    "\n",
    "    model.fc2.requires_grad_(False)\n",
    "    model.fc2_bn.requires_grad_(False)\n",
    "    #             model.upper_lower_concate_final.requires_grad_(False)\n",
    "\n",
    "\n",
    "    model.rcm_concate_fc1.requires_grad_(False)\n",
    "    model.rcm_concate_fc1_bn.requires_grad_(False)\n",
    "\n",
    "    model.rcm_concate_fc2.requires_grad_(False)\n",
    "    model.rcm_concate_fc2_bn.requires_grad_(False)\n",
    "    #             model.rcm_concate_final.requires_grad_(False)\n",
    "\n",
    "\n",
    "    model.to(device=device)\n",
    "\n",
    "    optimizer_name = 'Adam'\n",
    "    lr = 0.000212\n",
    "    l2_lambda = 2.355495e-08\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr, weight_decay=l2_lambda)\n",
    "    \n",
    "    epochs = 120 ### reduce the epochs from 150 to 100 to reduce the potential overfitting\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #         print(f\"I'am in the epoch {epoch}\")\n",
    "        model.train()\n",
    "        # record the training loss\n",
    "        running_loss = 0.0\n",
    "\n",
    "        ## deal with different number of features in different dataset with star* notation\n",
    "        for *features, train_labels in train_loader:\n",
    "            ### this line is just for nn.CrossEntropy loss otherwise can be safely removed\n",
    "            train_labels = train_labels.type(torch.LongTensor)\n",
    "\n",
    "            train_labels = train_labels.to(device)\n",
    "            features = [i.to(device) for i in features]\n",
    "\n",
    "            # forward pass\n",
    "            train_preds = model(*features)\n",
    "            loss = criterion(train_preds, train_labels)\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()  # empty the gradient from last round\n",
    "\n",
    "            # calculate the gradient\n",
    "            loss.backward()\n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        #         print(f\"I'am finished the epoch {epoch} training\")\n",
    "        ## start model validation\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # first evaluate the training acc ## don't need to evaluate the training acc\n",
    "            correct, total = 0.0, 0.0\n",
    "            for *features, train_labels in train_loader:\n",
    "                ### this type conversion is just used for nn.CrossEntropy loss\n",
    "                ### otherwise can be safely removed\n",
    "                train_labels = train_labels.to(device)\n",
    "                features = [i.to(device) for i in features]\n",
    "            \n",
    "                # get the predition with the model parameters updated after each epoch\n",
    "                preds = model(*features)\n",
    "                # prediction for the nn.CrossEntropy loss\n",
    "                _, preds_labels = torch.max(preds, 1)\n",
    "                correct += (preds_labels == train_labels).sum().item()\n",
    "                total += train_labels.shape[0]\n",
    "            \n",
    "            train_acc = round(correct / total, 4)\n",
    "            \n",
    "        print(f\"I'am finished the epoch {epoch} evaluation on the training set\")\n",
    "            \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f'epoch {epoch + 1}, training loss {running_loss}, train accuracy {train_acc}')\n",
    "    \n",
    "    # save the model at the end of 150 epochs\n",
    "    model_path = f\"{model_folder}/retrained_model_{epoch}.pt\"\n",
    "    \n",
    "    torch.save(model, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9d51607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Base1_RCM_triCNN_combined_9000_retraining():\n",
    "    ### where to save the 3-fold CV validation acc\n",
    "\n",
    "    ### where to save the retrained model\n",
    "    model_folder = '/home/wangc90/circRNA/circRNA_Data/model_outputs/Combined_model1_retraining/Combined_model1_retraining_9000'\n",
    "    \n",
    "    BS_LS_coordinates_path = '/home/wangc90/circRNA/circRNA_Data/BS_LS_data/updated_data/BS_LS_coordinates_final.csv'\n",
    "    hg19_seq_dict_json_path = '/home/wangc90/circRNA/circRNA_Data/hg19_seq/hg19_seq_dict.json'\n",
    "    flanking_dict_folder = '/home/wangc90/circRNA/circRNA_Data/BS_LS_data/flanking_dicts/'\n",
    "    bs_ls_dataset = BS_LS_DataSet_Prep(BS_LS_coordinates_path=BS_LS_coordinates_path,\n",
    "                               hg19_seq_dict_json_path=hg19_seq_dict_json_path,\n",
    "                               flanking_dict_folder=flanking_dict_folder,\n",
    "                               flanking_junction_bps=100,\n",
    "                               flanking_intron_bps=5000,\n",
    "                               training_size=9000)\n",
    "\n",
    "\n",
    "    ## generate the junction and flanking intron dict\n",
    "    bs_ls_dataset.get_junction_flanking_intron_seq()\n",
    "    \n",
    "    _, train_key2, test_keys = bs_ls_dataset.get_train_test_keys()\n",
    "\n",
    "    rcm_scores_folder = '/home/wangc90/circRNA/circRNA_Data/BS_LS_data/flanking_dicts/rcm_scores/'\n",
    "    # try without rcm features\n",
    "    train_torch_upper_lower_features, train_torch_flanking_rcm, train_torch_upper_rcm,\\\n",
    "    train_torch_lower_rcm, train_torch_labels = bs_ls_dataset.seq_to_tensor(data_keys=train_key2,\n",
    "                                                                            rcm_folder=rcm_scores_folder,\n",
    "                                                                            is_rcm=True, \n",
    "                                                                            is_upper_lower_concat=True)\n",
    "\n",
    "    BS_LS_dataset = BS_LS_upper_lower_concat_rcm(include_rcm=True,\n",
    "                                          seq_upper_lower_feature=train_torch_upper_lower_features,\n",
    "                                          flanking_rcm=train_torch_flanking_rcm,\n",
    "                                          upper_rcm=train_torch_upper_rcm,\n",
    "                                          lower_rcm=train_torch_lower_rcm,\n",
    "                                          label=train_torch_labels)\n",
    "    \n",
    "\n",
    "\n",
    "    print(len(BS_LS_dataset))\n",
    "\n",
    "    retraining(model=Base1_RCM_triCNN_combined_9000, dataset=BS_LS_dataset,model_folder=model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da91bfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr5|138837130|138837392|- has N in the extracted junctions, belongs to BS\n",
      "There are 0 overlapped flanking sequence from BS and LS  \n",
      "There are 7 repeated BS sequences\n",
      "There are 2 repeated LS sequences\n",
      "4000\n",
      "Loading the trained model weights to the Base1_RCM_triCNN_combined_9000 model\n",
      "I'am finished the epoch 0 evaluation on the training set\n",
      "I'am finished the epoch 1 evaluation on the training set\n",
      "I'am finished the epoch 2 evaluation on the training set\n",
      "I'am finished the epoch 3 evaluation on the training set\n",
      "I'am finished the epoch 4 evaluation on the training set\n",
      "I'am finished the epoch 5 evaluation on the training set\n",
      "I'am finished the epoch 6 evaluation on the training set\n",
      "I'am finished the epoch 7 evaluation on the training set\n",
      "I'am finished the epoch 8 evaluation on the training set\n",
      "I'am finished the epoch 9 evaluation on the training set\n",
      "I'am finished the epoch 10 evaluation on the training set\n",
      "I'am finished the epoch 11 evaluation on the training set\n",
      "I'am finished the epoch 12 evaluation on the training set\n",
      "I'am finished the epoch 13 evaluation on the training set\n",
      "I'am finished the epoch 14 evaluation on the training set\n",
      "I'am finished the epoch 15 evaluation on the training set\n",
      "I'am finished the epoch 16 evaluation on the training set\n",
      "I'am finished the epoch 17 evaluation on the training set\n",
      "I'am finished the epoch 18 evaluation on the training set\n",
      "I'am finished the epoch 19 evaluation on the training set\n",
      "I'am finished the epoch 20 evaluation on the training set\n",
      "I'am finished the epoch 21 evaluation on the training set\n",
      "I'am finished the epoch 22 evaluation on the training set\n",
      "I'am finished the epoch 23 evaluation on the training set\n",
      "I'am finished the epoch 24 evaluation on the training set\n",
      "I'am finished the epoch 25 evaluation on the training set\n",
      "I'am finished the epoch 26 evaluation on the training set\n",
      "I'am finished the epoch 27 evaluation on the training set\n",
      "I'am finished the epoch 28 evaluation on the training set\n",
      "I'am finished the epoch 29 evaluation on the training set\n",
      "I'am finished the epoch 30 evaluation on the training set\n",
      "I'am finished the epoch 31 evaluation on the training set\n",
      "I'am finished the epoch 32 evaluation on the training set\n",
      "I'am finished the epoch 33 evaluation on the training set\n",
      "I'am finished the epoch 34 evaluation on the training set\n",
      "I'am finished the epoch 35 evaluation on the training set\n",
      "I'am finished the epoch 36 evaluation on the training set\n",
      "I'am finished the epoch 37 evaluation on the training set\n",
      "I'am finished the epoch 38 evaluation on the training set\n",
      "I'am finished the epoch 39 evaluation on the training set\n",
      "I'am finished the epoch 40 evaluation on the training set\n",
      "I'am finished the epoch 41 evaluation on the training set\n",
      "I'am finished the epoch 42 evaluation on the training set\n",
      "I'am finished the epoch 43 evaluation on the training set\n",
      "I'am finished the epoch 44 evaluation on the training set\n",
      "I'am finished the epoch 45 evaluation on the training set\n",
      "I'am finished the epoch 46 evaluation on the training set\n",
      "I'am finished the epoch 47 evaluation on the training set\n",
      "I'am finished the epoch 48 evaluation on the training set\n",
      "I'am finished the epoch 49 evaluation on the training set\n",
      "epoch 50, training loss 20.558426022529602, train accuracy 0.8865\n",
      "I'am finished the epoch 50 evaluation on the training set\n",
      "I'am finished the epoch 51 evaluation on the training set\n",
      "I'am finished the epoch 52 evaluation on the training set\n",
      "I'am finished the epoch 53 evaluation on the training set\n",
      "I'am finished the epoch 54 evaluation on the training set\n",
      "I'am finished the epoch 55 evaluation on the training set\n",
      "I'am finished the epoch 56 evaluation on the training set\n",
      "I'am finished the epoch 57 evaluation on the training set\n",
      "I'am finished the epoch 58 evaluation on the training set\n",
      "I'am finished the epoch 59 evaluation on the training set\n",
      "I'am finished the epoch 60 evaluation on the training set\n",
      "I'am finished the epoch 61 evaluation on the training set\n",
      "I'am finished the epoch 62 evaluation on the training set\n",
      "I'am finished the epoch 63 evaluation on the training set\n",
      "I'am finished the epoch 64 evaluation on the training set\n",
      "I'am finished the epoch 65 evaluation on the training set\n",
      "I'am finished the epoch 66 evaluation on the training set\n",
      "I'am finished the epoch 67 evaluation on the training set\n",
      "I'am finished the epoch 68 evaluation on the training set\n",
      "I'am finished the epoch 69 evaluation on the training set\n",
      "I'am finished the epoch 70 evaluation on the training set\n",
      "I'am finished the epoch 71 evaluation on the training set\n",
      "I'am finished the epoch 72 evaluation on the training set\n",
      "I'am finished the epoch 73 evaluation on the training set\n",
      "I'am finished the epoch 74 evaluation on the training set\n",
      "I'am finished the epoch 75 evaluation on the training set\n",
      "I'am finished the epoch 76 evaluation on the training set\n",
      "I'am finished the epoch 77 evaluation on the training set\n",
      "I'am finished the epoch 78 evaluation on the training set\n",
      "I'am finished the epoch 79 evaluation on the training set\n",
      "I'am finished the epoch 80 evaluation on the training set\n",
      "I'am finished the epoch 81 evaluation on the training set\n",
      "I'am finished the epoch 82 evaluation on the training set\n",
      "I'am finished the epoch 83 evaluation on the training set\n",
      "I'am finished the epoch 84 evaluation on the training set\n",
      "I'am finished the epoch 85 evaluation on the training set\n",
      "I'am finished the epoch 86 evaluation on the training set\n",
      "I'am finished the epoch 87 evaluation on the training set\n",
      "I'am finished the epoch 88 evaluation on the training set\n",
      "I'am finished the epoch 89 evaluation on the training set\n",
      "I'am finished the epoch 90 evaluation on the training set\n",
      "I'am finished the epoch 91 evaluation on the training set\n",
      "I'am finished the epoch 92 evaluation on the training set\n",
      "I'am finished the epoch 93 evaluation on the training set\n",
      "I'am finished the epoch 94 evaluation on the training set\n",
      "I'am finished the epoch 95 evaluation on the training set\n",
      "I'am finished the epoch 96 evaluation on the training set\n",
      "I'am finished the epoch 97 evaluation on the training set\n",
      "I'am finished the epoch 98 evaluation on the training set\n",
      "I'am finished the epoch 99 evaluation on the training set\n",
      "epoch 100, training loss 19.99713821709156, train accuracy 0.8918\n",
      "I'am finished the epoch 100 evaluation on the training set\n",
      "I'am finished the epoch 101 evaluation on the training set\n",
      "I'am finished the epoch 102 evaluation on the training set\n",
      "I'am finished the epoch 103 evaluation on the training set\n",
      "I'am finished the epoch 104 evaluation on the training set\n",
      "I'am finished the epoch 105 evaluation on the training set\n",
      "I'am finished the epoch 106 evaluation on the training set\n",
      "I'am finished the epoch 107 evaluation on the training set\n",
      "I'am finished the epoch 108 evaluation on the training set\n",
      "I'am finished the epoch 109 evaluation on the training set\n",
      "I'am finished the epoch 110 evaluation on the training set\n",
      "I'am finished the epoch 111 evaluation on the training set\n",
      "I'am finished the epoch 112 evaluation on the training set\n",
      "I'am finished the epoch 113 evaluation on the training set\n",
      "I'am finished the epoch 114 evaluation on the training set\n",
      "I'am finished the epoch 115 evaluation on the training set\n",
      "I'am finished the epoch 116 evaluation on the training set\n",
      "I'am finished the epoch 117 evaluation on the training set\n",
      "I'am finished the epoch 118 evaluation on the training set\n",
      "I'am finished the epoch 119 evaluation on the training set\n"
     ]
    }
   ],
   "source": [
    "Base1_RCM_triCNN_combined_9000_retraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968a1669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
